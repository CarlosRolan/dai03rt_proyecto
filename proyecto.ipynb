{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f749fa61",
   "metadata": {},
   "source": [
    "### FUNCIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce32a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import datetime\n",
    "from datetime import timedelta, date\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from itertools import chain\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747b8d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ACCESS_TOKEN = \"eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiI2MDkxNGNiOTI5OTI2OGM1ZWQ2NjcxN2UyYmM5MzkyNyIsIm5iZiI6MTc1Njk3NTgxMS41NzMsInN1YiI6IjY4Yjk1MmMzNDYwNDI1OGMwNThjZGM1ZiIsInNjb3BlcyI6WyJhcGlfcmVhZCJdLCJ2ZXJzaW9uIjoxfQ.T-AD7PBLVI5hKAyYiHrheP1xhlGpIBZxJVJq_WlzsGM\"\n",
    "BASE = \"https://api.themoviedb.org/3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c8bfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# FUNCION necesaria para que vaya\n",
    "def _make_session() -> requests.Session:\n",
    "    s = requests.Session()\n",
    "    # Default headers for all requests in this session\n",
    "    s.headers.update({\n",
    "        \"accept\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {ACCESS_TOKEN}\",\n",
    "    })\n",
    "    # Robust retries for transient errors / rate limits\n",
    "    retry = Retry(\n",
    "        total=6,\n",
    "        connect=3,\n",
    "        read=3,\n",
    "        backoff_factor=0.5,                  # 0.5, 1.0, 2.0 ... exponential backoff\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\"],\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry, pool_connections=100, pool_maxsize=100)\n",
    "    s.mount(\"https://\", adapter)\n",
    "    s.mount(\"http://\", adapter)\n",
    "    return s\n",
    "\n",
    "# Hace un request a un rango especifico con una pagina especifica\n",
    "def _discover_in_range(session: requests.Session, page: int, gte: date, lte: date):\n",
    "    url = f\"{BASE}/discover/movie\"\n",
    "    params = {\n",
    "        \"language\": \"es-ES\",\n",
    "        \"include_adult\": \"false\",\n",
    "        \"include_video\": \"false\",\n",
    "        \"primary_release_date.gte\": gte.strftime(\"%Y-%m-%d\"),\n",
    "        \"primary_release_date.lte\": lte.strftime(\"%Y-%m-%d\"),\n",
    "        \"page\":page,\n",
    "    }\n",
    "    resp = session.get(url, params=params, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    ids = [m[\"id\"] for m in data.get(\"results\", [])]\n",
    "    return int(data.get(\"total_pages\", 0)), ids\n",
    "\n",
    "def get_total_ids_movies():\n",
    "    rangos_step = [\n",
    "        (date(2005, 1, 1),  date(2010, 12, 31), 182),\n",
    "        (date(2011, 1, 1),  date(2013, 9, 16),  120),\n",
    "        (date(2013, 9, 17), date(2014, 1, 6),   100),\n",
    "        (date(2014, 1, 7),  date(2015, 12, 31), 110),\n",
    "        (date(2016, 1, 1),  date(2021, 12, 31), 70),\n",
    "        (date(2022, 1, 1),  date(2024, 10, 16), 60),\n",
    "        (date(2024, 10, 17),date(2024, 12, 16), 30),\n",
    "        (date(2024, 12, 17),date.today(),       60),\n",
    "    ]\n",
    "\n",
    "    all_ids = []\n",
    "    total_pages_sum = 0\n",
    "    total_requests_sum = 0\n",
    "\n",
    "    with _make_session() as session:  # ← here is your `with` Session\n",
    "        for start, end, step in rangos_step:\n",
    "            print(f\"--- RANGO [{start} .. {end}] step={step}d\")\n",
    "            cur = start\n",
    "            pages_sum_range = 0\n",
    "            requests_sum_range = 0\n",
    "\n",
    "            while cur <= end:\n",
    "                # Inclusive window [cur, win_end]\n",
    "                win_end = min(cur + timedelta(days=step) - timedelta(days=1), end)\n",
    "\n",
    "                # Page 1 (also tells total_pages)\n",
    "                total_pages_1, ids_1 = _discover_in_range(session, 1, cur, win_end)\n",
    "                requests_sum_range += 1\n",
    "                pages_sum_range += min(total_pages_1, 500)\n",
    "                \n",
    "                ids_all = [ids_1] \n",
    "\n",
    "                print(f\" {cur}..{win_end} devuelve {total_pages_1} (>500)?\")\n",
    "\n",
    "                # Fetch remaining pages in parallel (if any)\n",
    "                #TODO OJO\n",
    "                if False:\n",
    "                    rem_pages = range(2, min(total_pages_1, 500) + 1)\n",
    "                    with ThreadPoolExecutor(max_workers=8) as ex:\n",
    "                        futures = {\n",
    "                            ex.submit(_discover_in_range, session, p, cur, win_end): p\n",
    "                            for p in rem_pages\n",
    "                        }\n",
    "                        for fut in as_completed(futures):\n",
    "                            _, ids = fut.result()\n",
    "                            ids_all.append(ids)\n",
    "                            requests_sum_range += 1\n",
    "                \n",
    "                all_ids.extend(chain.from_iterable(ids_all))\n",
    "                \n",
    "\n",
    "                # next window (avoid overlap, bounds are inclusive)\n",
    "                cur = win_end + timedelta(days=1)\n",
    "\n",
    "            print(\"Páginas toales en rango:\", pages_sum_range)\n",
    "            print(\"Requests totales en rango:\", requests_sum_range)\n",
    "            total_pages_sum += pages_sum_range\n",
    "            total_requests_sum += requests_sum_range\n",
    "\n",
    "    print(f\"Páginas totales los rangos: {total_pages_sum} (20 items/página)\")\n",
    "    print(f\"Requests totales en: {total_requests_sum}\")\n",
    "    return all_ids\n",
    "\n",
    "\n",
    "def _get_detail(session: requests.Session, id: int):\n",
    "    url = f\"{BASE}/movie/{id}\"\n",
    "    params = {\n",
    "        #\"language\": language,\n",
    "        \"append_to_response\": \"credits,release_dates,videos,keywords\",  # opcional\n",
    "    }\n",
    "    resp = session.get(url, params=params, timeout=30)\n",
    "    if resp.status_code == 404:\n",
    "        return None\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()\n",
    "\n",
    "def _detail_worker(mid):\n",
    "    with _make_session() as s:\n",
    "        return _get_detail(s, mid)\n",
    "    \n",
    "def fetch_details_for_ids(ids):\n",
    "\n",
    "    results = [None] * len(ids)\n",
    "    with ThreadPoolExecutor(max_workers=12) as ex:\n",
    "        futures = {ex.submit(_detail_worker, mid): i\n",
    "                       for i, mid in enumerate(ids)}\n",
    "        for fut in as_completed(futures):\n",
    "            i = futures[fut]\n",
    "            try:\n",
    "                results[i] = fut.result()\n",
    "            except requests.HTTPError as e:\n",
    "                # Si quieres conservar el fallo:\n",
    "                results[i] = {\"id\": ids[i], \"_error\": True, \"status\": getattr(e.response, \"status_code\", None)}\n",
    "            except Exception as e:\n",
    "                results[i] = {\"id\": ids[i], \"_error\": True, \"exception\": str(e)}\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845dfbd5",
   "metadata": {},
   "source": [
    "### FUNCIONES AUXILIARES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb533594",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_movies_ids = get_total_ids_movies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a7a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_movies_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a661249",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = random.sample(all_movies_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71a7c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = fetch_details_for_ids(sample)\n",
    "movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ed0901",
   "metadata": {},
   "source": [
    "# Métrica de éxito\n",
    "success = 1 if revenue > budget and vote_average >= 7 else 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36198389",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_movies_sucia = pd.DataFrame(movies)\n",
    "df_movies = df_movies_sucia.copy()\n",
    "df_movies_sucia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac4f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_with_median(df):\n",
    "    median = df[df[\"budget\"] != 0][\"budget\"].median()\n",
    "\n",
    "    df[\"budget\"] = [median if b == 0 else b for b in df[\"budget\"]]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_success_column(df):\n",
    "    if 'revenue' in df.columns and 'budget' in df.columns:\n",
    "        df[\"succes\"] = (df_movies[\"revenue\"] > df_movies[\"budget\"]) & (df_movies[\"vote_average\"] > 7)\n",
    "    else:\n",
    "        raise ValueError(\"El DataFrame debe contener las columnas 'revenue' y 'budget'\")\n",
    "    return df\n",
    "\n",
    "def prepare_table_for_sql(df):\n",
    "    df_movies_original = df.copy()\n",
    "    df = df_movies_original[[\"id\", \"title\",\"popularity\", \"vote_average\", \"runtime\", \"budget\", \"revenue\", \"overview\", \"genres\", \"credits\", \"release_date\"]]\n",
    "    df[\"credits\"] = [get_values_for_row(credits[\"cast\"], \"id\") for credits in df_movies_original[\"credits\"]]\n",
    "    df[\"genres\"] = [get_values_for_row(genres, \"id\") for genres in df_movies_original[\"genres\"]]\n",
    "    df = add_success_column(df)\n",
    "    df = impute_with_median(df)\n",
    "    return df\n",
    "\n",
    "def prepare_table_for_ia(df):\n",
    "    return  df[[\"popularity\",\"vote_average\",\"runtime\",\"budget\", \"succes\"]]\n",
    "\n",
    "\n",
    "def get_people_from_movie(movie_id, df):\n",
    "    movie_data = df[df[\"id\"] == movie_id]\n",
    "    return movie_data\n",
    "\n",
    "# Para las columnas de CREDITS y GENRES donde nos vienen la información que relaciona la\n",
    "# tabla películas con actores y generos\n",
    "def get_values_for_row(items, column_name):\n",
    "    return [item.get(column_name) for item in items if isinstance(item, dict) and column_name in item]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4659566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import date, datetime\n",
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "\n",
    "def df_to_movies_insert(df: pd.DataFrame, table: str = \"public.movies\") -> str:\n",
    "    required = [\n",
    "        \"id\", \"title\", \"popularity\", \"vote_average\", \"runtime\",\n",
    "        \"budget\", \"revenue\", \"overview\", \"release_date\", \"success\"\n",
    "    ]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required column(s): {missing}\")\n",
    "\n",
    "    def esc(s: str) -> str:\n",
    "        # Escape single quotes for SQL strings\n",
    "        return s.replace(\"'\", \"''\")\n",
    "\n",
    "    def fmt_str(s) -> str:\n",
    "        if pd.isna(s) or s is None:\n",
    "            return \"''\"\n",
    "        return f\"'{esc(str(s))}'\"\n",
    "\n",
    "    def fmt_int(x) -> str:\n",
    "        if pd.isna(x) or x is None:\n",
    "            return \"NULL\"\n",
    "        return str(int(x))\n",
    "\n",
    "    def fmt_dec(x, decimals: int) -> str:\n",
    "        # Use Decimal to avoid float artifacts and force fixed decimals\n",
    "        if pd.isna(x) or x is None:\n",
    "            return \"NULL\"\n",
    "        q = Decimal(\"1\").scaleb(-decimals)  # e.g., decimals=4 -> Decimal('0.0001')\n",
    "        val = Decimal(str(x)).quantize(q, rounding=ROUND_HALF_UP)\n",
    "        return f\"{val:.{decimals}f}\"\n",
    "\n",
    "    def fmt_date(d) -> str:\n",
    "        if pd.isna(d) or d is None:\n",
    "            return \"NULL\"\n",
    "        if isinstance(d, (datetime, date)):\n",
    "            return f\"DATE '{d.strftime('%Y-%m-%d')}'\"\n",
    "        s = str(d).strip()\n",
    "        if not s:\n",
    "            return \"NULL\"\n",
    "        try:\n",
    "            ymd = s[:10]\n",
    "            datetime.strptime(ymd, \"%Y-%m-%d\")\n",
    "            return f\"DATE '{ymd}'\"\n",
    "        except Exception:\n",
    "            return \"NULL\"\n",
    "\n",
    "    def fmt_bool(b) -> str:\n",
    "        if pd.isna(b) or b is None:\n",
    "            return \"FALSE\"\n",
    "        return \"TRUE\" if bool(b) else \"FALSE\"\n",
    "\n",
    "    values = []\n",
    "    for _, r in df.iterrows():\n",
    "        row_sql = (\n",
    "            fmt_int(r[\"id\"]),                    # id\n",
    "            fmt_str(r[\"title\"]),                 # title\n",
    "            fmt_dec(r[\"popularity\"], 4),         # popularity\n",
    "            fmt_dec(r[\"vote_average\"], 1),       # vote_average\n",
    "            fmt_int(r[\"runtime\"]),               # runtime\n",
    "            fmt_int(r[\"budget\"]),                # budget\n",
    "            fmt_int(r[\"revenue\"]),               # revenue\n",
    "            fmt_str(r[\"overview\"] if \"overview\" in r else \"\"),  # overview\n",
    "            fmt_date(r[\"release_date\"]),         # release_date\n",
    "            fmt_bool(r[\"success\"])               # success\n",
    "        )\n",
    "        values.append(f\"({', '.join(row_sql)})\")\n",
    "\n",
    "    if not values:\n",
    "        raise ValueError(\"DataFrame is empty; no rows to insert.\")\n",
    "\n",
    "    head = (\n",
    "        f\"INSERT INTO {table} (\\n\"\n",
    "        f\"  id, title, popularity, vote_average, runtime, budget, revenue, overview, release_date, success\\n\"\n",
    "        f\") VALUES\\n\"\n",
    "    )\n",
    "    tail = \"\\nON CONFLICT (id) DO NOTHING;\"\n",
    "    return head + \",\\n\".join(values) + tail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128b25f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = prepare_table_for_sql(df_movies)\n",
    "df_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57347ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = df_movies[[\"popularity\",\"vote_average\",\"runtime\",\"budget\", \"success\"]]\n",
    "data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ea11b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import sklearn # Para ver la versión\n",
    "\n",
    "# Escalado\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Train, Test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Metricas\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Clasificadores\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Validacion\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def save_model(clf, scaler):\n",
    "    payload = {\n",
    "        \"scaler\": scaler,            # fitted StandardScaler\n",
    "        \"classifier\": clf,    # fitted LogisticRegression\n",
    "    }\n",
    "    joblib.dump(payload, \"model_and_scaler.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f52f06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data_set.drop(\"succes\", axis=1)\n",
    "y = data_set[\"succes\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ce27b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dict = {k: v.to_dict() for k, v in X_train.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa0cc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame(clean_dict)[\"budget\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95df013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0224e194",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef16ba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=42, max_iter=10_000)\n",
    "\n",
    "clf.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd63d5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(clf=clf, scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9a4729",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "y_proba = clf.predict_proba(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee9113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Metricas\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "f1score_ = f1_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average=None)\n",
    "roc_auc = roc_auc_score(y_test, y_proba[:,1])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac9280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "confusion_matrix_ = sns.heatmap(data=cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b33e532",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_proba.min(), y_proba.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fef767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "sns.lineplot(x=fpr, y=tpr, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "sns.lineplot(x=[0,1], y=[0,1], color=\"red\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Curva ROC\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57177cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc)\n",
    "print(cm)\n",
    "print(report)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce1d062",
   "metadata": {},
   "source": [
    "### Procesamiento de datos pre split\n",
    "- lower case\n",
    "- quedarme solo con el año\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7211d733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "bundle = joblib.load(\"model_and_scaler.joblib\")\n",
    "scaler = bundle[\"scaler\"]\n",
    "clf    = bundle[\"classifier\"]\n",
    "\n",
    "X_raw = np.asarray([[...]], dtype=float)\n",
    "X_scaled = scaler.transform(X_raw)\n",
    "y_pred  = clf.predict(X_scaled)\n",
    "y_proba = clf.predict_proba(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4585a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_model(nombre_del_archivo):\n",
    "    bundle = joblib.load(nombre_del_archivo)\n",
    "    scaler = bundle[\"scaler\"]\n",
    "    clf    = bundle[\"classifier\"]\n",
    "    return clf, scaler\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17cac33",
   "metadata": {},
   "source": [
    "### TRAIN TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2aba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data_set.drop(\"succes\", axis=1)\n",
    "y = data_set[\"succes\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7703810",
   "metadata": {},
   "source": [
    "### Procesamiento post split\n",
    "- bag of words para overview\n",
    "- bago of words para title\n",
    "- escalado de variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb65218",
   "metadata": {},
   "source": [
    "\n",
    "### Claves para que se vea bien en `/docs`\n",
    "- **`summary` y `description`** en cada `@app.get/post` → el texto aparece arriba; `description` acepta **Markdown**.\n",
    "- **`tags`** → agrupa endpoints en Swagger.\n",
    "- **`Query(..., description=..., examples=...)`** para documentar **query params** con ejemplos.\n",
    "- **`Body(..., openapi_examples=...)`** para documentar **el body** con **varios ejemplos** (ideal para *lista de diccionarios*).\n",
    "- **`response_model=...`** para que Swagger muestre la **forma de la respuesta**.\n",
    "- Convierte **NumPy → list** con `.tolist()` antes de devolver, para que sea JSON.\n",
    "\n",
    "Con eso, al abrir `http://127.0.0.1:8000/docs` verás cada endpoint con su resumen, descripción, ejemplos de entrada y el esquema de salida. ¿Quieres que te adapte el `feature_order` a tus columnas reales del scaler?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e7ae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "BUKECT_NAME = \"dai03rt-proyecto\"\n",
    "ACCESS_KEY_ID = \"AKIAWZEDMKF3SHFDRH3B\"\n",
    "SECRET_KEY = \"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\"\n",
    "REGION = \"eu-west-1\"\n",
    "\n",
    "s3 = boto3.client(\"s3\",aws_access_key_id = ACCESS_KEY_ID,aws_secret_access_key = SECRET_KEY,region_name=REGION) \n",
    "\n",
    "lambda_ = boto3.client(\"lambda\",\n",
    "                  aws_access_key_id = ACCESS_KEY_ID,\n",
    "                  aws_secret_access_key = SECRET_KEY,\n",
    "                  region_name=REGION) \n",
    "\n",
    "def invocar_lambda(nombre_funcion, payload={}):\n",
    "    try:\n",
    "        response = lambda_.invoke(\n",
    "            FunctionName=nombre_funcion,\n",
    "            InvocationType='RequestResponse',\n",
    "            Payload=json.dumps(payload),\n",
    "        )\n",
    "        print(\"Respuesta:\")\n",
    "        result_raw = (response['Payload']).read().decode('utf-8')\n",
    "        result = json.loads(result_raw)\n",
    "        print(result)\n",
    "        return result\n",
    "    except ClientError as e:\n",
    "        print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27677074",
   "metadata": {},
   "source": [
    "### ENDPOINTS PREDICT, ASK-TEXT Y ASK-VISUAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7ca8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "BUKECT_NAME = \"dai03rt-proyecto\"\n",
    "ACCESS_KEY_ID = \"AKIAWZEDMKF3SHFDRH3B\"\n",
    "SECRET_KEY = \"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\"\n",
    "REGION = \"eu-west-1\"\n",
    "\n",
    "s3 = boto3.client(\"s3\",aws_access_key_id = ACCESS_KEY_ID,aws_secret_access_key = SECRET_KEY,region_name=REGION) \n",
    "\n",
    "lambda_ = boto3.client(\"lambda\",\n",
    "                  aws_access_key_id = ACCESS_KEY_ID,\n",
    "                  aws_secret_access_key = SECRET_KEY,\n",
    "                  region_name=REGION) \n",
    "\n",
    "def invocar_lambda(nombre_funcion, payload={}):\n",
    "    try:\n",
    "        response = lambda_.invoke(\n",
    "            FunctionName=nombre_funcion,\n",
    "            InvocationType='RequestResponse',\n",
    "            Payload=json.dumps(payload),\n",
    "        )\n",
    "        print(\"Respuesta:\")\n",
    "        result_raw = (response['Payload']).read().decode('utf-8')\n",
    "        result = json.loads(result_raw)\n",
    "        print(result)\n",
    "        return result\n",
    "    except ClientError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "# FAST_API\n",
    "# IMPORTS Y CONFIGURACION\n",
    "\n",
    "import io\n",
    "import os\n",
    "from typing import Dict, List\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from fastapi import Body, FastAPI\n",
    "from fastapi import Query\n",
    "from fastapi.responses import StreamingResponse\n",
    "from fastapi.testclient import TestClient\n",
    "from google import genai\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os, sys, json, base64, tempfile, subprocess\n",
    "from pathlib import Path\n",
    "from fastapi import FastAPI, Request, Query, HTTPException\n",
    "from fastapi.responses import HTMLResponse, JSONResponse, PlainTextResponse\n",
    "import json\n",
    "\n",
    "def get_config_SQL_GRAPH(question, rows_literal, query):\n",
    "    return f\"\"\"Eres un generador determinista de código Python. No ejecutes SQL ni leas archivos. Recibirás:\n",
    "- SQL (solo contexto)\n",
    "- ROWS: una lista de diccionarios en sintaxis Python (o JSON equivalente) donde cada diccionario es una fila (vista ya materializada)\n",
    "- GOAL (la pregunta original del usuario)\n",
    "\n",
    "Tu tarea: generar SOLO código Python ejecutable que:\n",
    "1) Importe: pandas (pd), seaborn (sns), matplotlib.pyplot (plt). Evita imports innecesarios.\n",
    "2) Construya un DataFrame a partir de ROWS insertando el literal EXACTO dentro del código:\n",
    "   - Define: ROWS = {rows_literal}\n",
    "   - df = pd.DataFrame(ROWS)\n",
    "   - No uses eval() ni ast.literal_eval(); no parses cadenas: inserta el literal tal cual.\n",
    "3) Si el DataFrame está vacío, imprime un mensaje claro y termina sin error.\n",
    "4) Tipos:\n",
    "   - Convierte a datetime columnas con nombres como 'release_date','date','created_at','updated_at' con pd.to_datetime(errors='coerce').\n",
    "   - Convierte a numérico columnas que parezcan numéricas con pd.to_numeric(errors='coerce').\n",
    "   - Trata como categóricas columnas de texto con baja cardinalidad.\n",
    "5) Selección de visualización (máx 2 gráficos), **guiada por GOAL + SQL + columnas del DataFrame**. Sigue este orden de decisión y elige la opción más informativa:\n",
    "   0) **KPI escalar (conteos/totales):** si el resultado es 1×1, o GOAL/SQL indiquen conteo/total (palabras como 'cuántas/os', 'how many', 'count', 'total', o columnas 'count','total','cantidad','num_*'):\n",
    "      - Dibuja una tarjeta KPI con matplotlib (texto grande centrado, p.ej. fontsize 64) y subtítulo breve con GOAL. Guarda 'chart1.png'. No hagas histogramas triviales.\n",
    "   a) **Tendencia temporal:** si hay columna fecha o SQL agrupa por fecha y GOAL pide evolución/“a lo largo del tiempo”/“por mes/año”:\n",
    "      - Agrega por periodo razonable (mes si rango>90 días; si no, día). Dibuja lineplot del conteo y, si existe métrica típica ('popularity','revenue','vote_average','budget','runtime'), también su media (2º gráfico).\n",
    "   b) **Ranking / Top-N / Bottom-N:** si GOAL pide “top”, “más/menos”, “ranking” sobre una categórica y una numérica:\n",
    "      - Calcula la métrica (sum/media según SQL o por defecto sum). Ordena desc y muestra **Top-N=20** (o Bottom-N si aplica) con barplot.\n",
    "   c) **Comparación por categorías:** si GOAL pide “por género/actor/...”: barplot por categórica (Top-20). Si hay una segunda categórica muy relevante, usa hue limitado o facetas moderadas (máximo 6 categorías en hue/facetas).\n",
    "   d) **Relación entre métricas:** si GOAL sugiere “relación/correlación/X vs Y”, o el SQL selecciona ≥2 numéricas relevantes:\n",
    "      - Scatterplot (sns.scatterplot o regplot). Muestra sample de máx 1.000 filas si hay muchas.\n",
    "   e) **Distribución:** si GOAL pide “distribución/variabilidad” o solo hay 1 numérica útil:\n",
    "      - histplot si la columna tiene ≥15 valores distintos y NO parece un ID. Alternativamente, boxplot.\n",
    "   f) **Composición / proporciones:** si GOAL habla de “porcentaje/proporción/cuota/desglose” entre dos categóricas o categórica+num:\n",
    "      - Barplot apilado normalizado (por categoría base) o heatmap de tabla de contingencia (counts o medias).\n",
    "   g) **1 fila con >1 numéricas:** barplot horizontal de esa única fila, ordenado por valor.\n",
    "   h) Si ninguna opción produce una visualización informativa, responde EXACTAMENTE 'VIS_IMPOSIBLE'.\n",
    "\n",
    "6) Buenas prácticas:\n",
    "   - Títulos y ejes legibles; rota etiquetas si hay muchas categorías.\n",
    "   - Evita pie charts. Evita histogramas de columnas tipo id/movie_id/actor_id.\n",
    "   - Limita categorías a Top-20; para scatter/pairplots, sample máx 1.000 filas.\n",
    "   - Maneja NaN: descarta filas NaN de las columnas usadas para el gráfico.\n",
    "7) Guarda las figuras como 'chart1.png' y 'chart2.png' (si generas dos). Llama a plt.show() al final.\n",
    "8) No uses conexiones a bases de datos, variables de entorno ni lecturas/escrituras adicionales (salvo los PNG). No imprimas el SQL completo; puedes incluir un título breve con GOAL y/o una mención corta al contexto.\n",
    "9) Salida estricta: devuelve SOLO el código Python ejecutable o el texto EXACTO 'VIS_IMPOSIBLE'. No añadas explicación ni markdown.\n",
    "\n",
    "ENTRADA\n",
    "SQL:\n",
    "{query}\n",
    "\n",
    "ROWS (lista de dicts en sintaxis Python o JSON):\n",
    "{rows_literal}\n",
    "\n",
    "GOAL (pregunta original en lenguaje natural):\n",
    "{question}\n",
    "\n",
    "SALIDA\n",
    "- SOLO código Python ejecutable que cumpla las reglas, o el texto exacto 'VIS_IMPOSIBLE'.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_config_SQL_to_NL(question, db_view):\n",
    "    \n",
    "    return f\"\"\"Eres un asistente que transforma una tabla (pandas DataFrame en texto) en una explicación en lenguaje natural.\n",
    "\n",
    "                    Instrucciones:\n",
    "                    1) Responde a la PREGUNTA usando exclusivamente los datos de la TABLA proporcionada.\n",
    "                    2) Si la TABLA está vacía, responde de forma natural indicando que no hay resultados para la consulta.\n",
    "                    3) Si la PREGUNTA requiere información que no está en las columnas de la TABLA responde EXACTAMENTE:\n",
    "                    No tengo info suficiente para responder a esa pregunta con la tabla que estoy viendo\n",
    "                    4) Si en vez de tabla recibes el mensaje \"NOT_INFO\" responde EXACTAMENTE:\n",
    "                    La pregunta que me has hecho es imposible de contestar con los datos actuales de la base de datos\n",
    "                    5) No inventes datos. No incluyas código, SQL ni instrucciones técnicas.\n",
    "                    6) Sé claro y conciso. Resume tendencias (máximos/mínimos, medias, top-N) solo si ayudan a responder la PREGUNTA; no listes todas las filas salvo que la PREGUNTA lo pida o la TABLA tenga ≤ 10 filas.\n",
    "                    7) Mantén el idioma de la PREGUNTA original.\n",
    "                    8) Si mencionas columnas, usa sus nombres exactos.\n",
    "                    9) Se la input en vez de ser un select es \"Error\" responde EXACTAMENTE:\n",
    "                    La lambda \"send_query\" ha tenido un error de ejecucion y no ha podido generar la tabla para que te responda\n",
    "                    10) Si en vez de tabla recibes el mensaje \"LAMBDA_ERROR\":\n",
    "                    Parece que ha habido un error al intentar consultar nuestra base de datos, por favor intentalo de nuevo más tarde...\n",
    "                    Disculpa las molestias\n",
    "                    \n",
    "                    Entrada:\n",
    "                    PREGUNTA:\n",
    "                    {question}\n",
    "\n",
    "                    TABLA (proporcionada para responder a la pregunta):\n",
    "                    {db_view}\n",
    "\n",
    "                    Salida:\n",
    "                    - Solo un texto en lenguaje natural que responda a la PREGUNTA usando la TABLA, o uno de los mensajes indicados arriba.\n",
    "                    \"\"\"\n",
    "                    \n",
    "def get_config_NL_to_SQL(pregunta):\n",
    "    return f\"\"\"Eres un generador determinista de SQL. Convierte una pregunta en lenguaje natural en UNA ÚNICA sentencia SQL para la base de datos descrita abajo.\n",
    "\n",
    "                        Dialecto objetivo: PostgreSQL 15\n",
    "\n",
    "                        Esquema autorizado (tablas, columnas, tipos, claves). No asumas nada fuera de esto:\n",
    "                        {DB_SCHEMA}\n",
    "\n",
    "                        REGLAS OBLIGATORIAS\n",
    "                        1) SOLO puedes generar sentencias SELECT. Nunca generes INSERT, UPDATE, DELETE, DROP u otras operaciones que modifiquen datos o estructura.\n",
    "                        2) No inventes tablas, columnas, funciones ni relaciones no presentes en el esquema.\n",
    "                        3) Si la pregunta requiere datos o campos que NO existen en el esquema, responde EXACTAMENTE:\n",
    "                        SQL_IMPOSIBLE\n",
    "                        4) Produce SOLO una sentencia SQL SELECT terminada en punto y coma. Nada de explicaciones, comentarios, CTEs o múltiples sentencias si no son estrictamente necesarias.\n",
    "                        5) Usa JOINs explícitos y claves tal como figuran en el esquema. Evita SELECT *; nombra columnas relevantes.\n",
    "                        6) Para filtros de texto tipo “contiene” o búsquedas case-insensitive, usa ILIKE si procede.\n",
    "                        7) Para fechas relativas, solo si hay columnas de fecha en el esquema:\n",
    "                        - “últimos 7 días” → WHERE columna_fecha >= CURRENT_DATE - INTERVAL '7 days'\n",
    "                        - “este año” → date_trunc('year', columna_fecha) = date_trunc('year', CURRENT_DATE)\n",
    "                        8) Si la pregunta pide top/N, añade ORDER BY apropiado y LIMIT N.\n",
    "                        9) Si pide agregaciones, usa GROUP BY y agregados correctos.\n",
    "                        10) Si la pregunta es ambigua pero resoluble con el esquema, elige la interpretación más razonable por nombres/relaciones. Si sigue siendo irresoluble, devuelve el mensaje de insuficiencia.\n",
    "                        11) Salida estricta: devuelve SOLO la sentencia SQL o el mensaje de insuficiencia, sin texto adicional.\n",
    "                        12) Ten en cuenta que la pregunta puede pedir información sobre una entidad mal escrita; siempre corrígela y tradúcela al inglés.\n",
    "                            - Por ejemplo, si piden las películas de un actor pero han escrito mal el nombre, el SELECT no funcionará si usas el nombre tal cual.\n",
    "                            - Si no tienes la capacidad de saber por quién te están preguntando, di que no conoces a un actor con ese nombre (usa el mensaje de insuficiencia).\n",
    "                        13) Los géneros, sin embargo, son una lista corta limitada de estas posibilidades:\n",
    "                        [\"Action\",\"Adventure\",\"Animation\",\"Comedy\",\"Crime\",\"Documentary\",\"Drama\",\"Family\",\"Fantasy\",\"History\",\"Horror\",\"Music\",\"Mystery\",\"Romance\",\"Science Fiction\",\"TV Movie\",\"Thriller\",\"War\",\"Western\"]\n",
    "                            - Así que ya sabes qué valores utilizar cuando tengas que hacer alguna consulta que requiera un filtro o agrupación por género.\n",
    "                        14) Para nombres y géneros con erratas o en español:\n",
    "                            – No traduzcas nombres propios; genera un filtro robusto con ILIKE a partir de los tokens del nombre (p. ej., “Brada Pitte” → ILIKE '%brad%pitt%').\n",
    "                            – Mapea términos de género en español/sinónimos a la lista canónica permitida. Ejemplos: “terror/suspenso”→[\"Horror\",\"Thriller\"], “ciencia ficción/sci-fi”→[\"Science Fiction\"], “aventura”→[\"Adventure\"], “familiar”→[\"Family\"], “crimen”→[\"Crime\"], etc.\n",
    "                            – Si un término no está en el mapeo, responde SQL_IMPOSIBLE.\n",
    "                            – Recuerda que una película puede tener varios géneros; usa IN (...) sobre g.name.\n",
    "                        \n",
    "                        FORMATO DE ENTRADA (usuario)\n",
    "                        - Texto libre con la pregunta en lenguaje natural (tipo informativo, por ejemplo: \"dime las películas más famosas de este año\").\n",
    "\n",
    "                        FORMATO DE SALIDA (obligatorio)\n",
    "                        -Salida estricta: devuelve solo la sentencia SQL en texto plano, sin bloques de código, sin backticks, sin prefijos como sql. No añadas comentarios ni explicación. Termina en ;.\n",
    "                        - UNA sentencia SQL SELECT válida para PostgreSQL 15, terminada en ';'\n",
    "                        O\n",
    "                        - El texto exacto: SQL_IMPOSIBLE\n",
    "\n",
    "                        PREGUNTA\n",
    "                        {pregunta}\n",
    "                        \"\"\"\n",
    "\n",
    "# Contenido de tu archivo .sql como string\n",
    "DB_SCHEMA = \"\"\"\n",
    "-- SCHEMA: tmdb (tablas en 'public')\n",
    "\n",
    "-- TABLE: movies\n",
    "CREATE TABLE IF NOT EXISTS public.movies (\n",
    "    id              INTEGER PRIMARY KEY,                      -- TMDB movie id (manual)\n",
    "    title           TEXT NOT NULL,\n",
    "    popularity      REAL,                                     -- float4\n",
    "    vote_average    NUMERIC(3,1) \n",
    "    CHECK (vote_average >= 0 AND vote_average <= 10),     -- 0.0..10.0\n",
    "    runtime         SMALLINT CHECK (runtime >= 0),            -- minutos\n",
    "    budget          BIGINT CHECK (budget  >= 0),              -- entero (unidades monetarias)\n",
    "    revenue         BIGINT CHECK (revenue >= 0),\n",
    "    overview        TEXT,\n",
    "    release_date    DATE,\n",
    "    success         BOOLEAN\n",
    ");\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS idx_movies_release_date ON public.movies (release_date);\n",
    "CREATE INDEX IF NOT EXISTS idx_movies_popularity  ON public.movies (popularity);\n",
    "CREATE INDEX IF NOT EXISTS idx_movies_title_lower ON public.movies (lower(title));\n",
    "\n",
    "-- TABLE: actors (cast)\n",
    "CREATE TABLE IF NOT EXISTS public.actors (\n",
    "    id          INTEGER PRIMARY KEY,                          -- TMDB person id (manual)\n",
    "    name        TEXT NOT NULL,\n",
    "    age         SMALLINT CHECK (age BETWEEN 0 AND 150),\n",
    "    gender      SMALLINT CHECK (gender IN (0,1,2,3)),         -- TMDB: 0 unknown, 1 female, 2 male, 3 non-binary\n",
    "    popularity  REAL\n",
    ");\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS idx_actors_name_lower ON public.actors (lower(name));\n",
    "CREATE INDEX IF NOT EXISTS idx_actors_popularity ON public.actors (popularity);\n",
    "\n",
    "-- TABLE: genres\n",
    "CREATE TABLE IF NOT EXISTS public.genres (\n",
    "    id      INTEGER PRIMARY KEY,          -- TMDB genre id (manual)\n",
    "    name    TEXT NOT NULL UNIQUE\n",
    ");\n",
    "\n",
    "-- TABLE: movie_actors  (credits → cast, relación N:M)\n",
    "CREATE TABLE IF NOT EXISTS public.movie_actors (\n",
    "    movie_id INTEGER NOT NULL REFERENCES public.movies(id) ON DELETE CASCADE,\n",
    "    actor_id INTEGER NOT NULL REFERENCES public.actors(id) ON DELETE CASCADE,\n",
    "    PRIMARY KEY (movie_id, actor_id)\n",
    ");\n",
    "\n",
    "-- TABLE: movie_genres  (genres, relación N:M)\n",
    "CREATE TABLE IF NOT EXISTS public.movie_genres (\n",
    "    movie_id INTEGER NOT NULL REFERENCES public.movies(id) ON DELETE CASCADE,\n",
    "    genre_id INTEGER NOT NULL REFERENCES public.genres(id) ON DELETE CASCADE,\n",
    "    PRIMARY KEY (movie_id, genre_id)\n",
    ");\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS idx_movie_actors_actor ON public.movie_actors (actor_id);\n",
    "CREATE INDEX IF NOT EXISTS idx_movie_genres_genre ON public.movie_genres (genre_id);\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "API_KEY = \"AIzaSyBZrMh8zSQPiBEyM8vNuzpxv6ZXkD42x_Y\"\n",
    "MODEL = \"gemini-2.5-flash\"\n",
    "\n",
    "# Configurar Gemini\n",
    "client = genai.Client(api_key=API_KEY)\n",
    "\n",
    "# Crear la aplicación FastAPI\n",
    "app = FastAPI(\n",
    "    title=\"Movie Success API\",\n",
    "    description=\"API para predecir éxito de películas y responder preguntas\",\n",
    "    version=\"1.0\"\n",
    ")\n",
    "\n",
    "def _load_model(nombre_del_archivo):\n",
    "    bundle = joblib.load(nombre_del_archivo)\n",
    "    scaler = bundle[\"scaler\"]\n",
    "    clf    = bundle[\"classifier\"]\n",
    "    return clf, scaler\n",
    "\n",
    "def _ask_gemini(promt):\n",
    "    respuesta = client.models.generate_content(model = MODEL,\n",
    "                                               contents = [{\"parts\": [{\"text\": promt}]}])\n",
    "                                                \n",
    "    return respuesta.text\n",
    "\n",
    "@app.post(\"/predict\",\n",
    "    summary=\"Predecir éxito de películas\",\n",
    "    description=\"\"\"\n",
    "                Envía **una lista de diccionarios** con las features del modelo.\n",
    "                Las claves deben coincidir con lo que el `scaler` espera.\n",
    "                \"\"\")\n",
    "def predict(data: List[Dict[str, float]] = Body(\n",
    "        ...,\n",
    "        description=\"Lista de diccionarios con las features del modelo.\",\n",
    "        example=[\n",
    "            {\n",
    "                \"popularity\":0.9957,\n",
    "                \"vote_average\":7.1,\n",
    "                \"runtime\": 98,\n",
    "                \"budget\":9112.5\n",
    "            },\n",
    "            {\n",
    "                \"popularity\":0.4007,\n",
    "                \"vote_average\":5.1,\n",
    "                \"runtime\": 0,\n",
    "                \"budget\":9112.5\n",
    "            }\n",
    "        ]    \n",
    "    )\n",
    "):\n",
    "\n",
    "    try:\n",
    "        data = pd.DataFrame(data)\n",
    "        clf, scaler = _load_model(\"model_and_scaler.joblib\")\n",
    "        X_scaled = scaler.transform(data)\n",
    "        y_pred  = clf.predict(X_scaled)\n",
    "        y_proba = clf.predict_proba(X_scaled)\n",
    "        \n",
    "        result_body = {\n",
    "            \"y_pred\": y_pred,\n",
    "            \"y_proba\": y_proba\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            \"statuscode\": 200,\n",
    "            \"result\": result_body\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        return {\n",
    "            \"statuscode\": 500,\n",
    "            \"result\": error_msg\n",
    "        }\n",
    "        \n",
    "@app.get(\"/ask-text\")\n",
    "def ask_text(question: str = Query(..., min_length=2)):\n",
    "    \n",
    "    # Paso 1: humano -> SQL\n",
    "    \n",
    "    query = _ask_gemini(get_config_NL_to_SQL(question))\n",
    "\n",
    "    # Paso 2: tabla simulada\n",
    "    event = {\"QUERY\": query}\n",
    "    response = invocar_lambda(\"send_query\", event)\n",
    "    \n",
    "    if response[\"ok\"]:\n",
    "        table_view = response[\"result\"]\n",
    "    elif response[\"ok\"]== \"NOT_INFO\":\n",
    "        table_view = response[\"ok\"]\n",
    "    else:\n",
    "        table_view = \"LAMBDA_ERROR\"\n",
    "        \n",
    "\n",
    "    # Paso 3: tabla -> humano\n",
    "    \n",
    "    nl_response = _ask_gemini(get_config_SQL_to_NL(question, table_view))\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"query\": query,\n",
    "        \"table_view\": table_view,\n",
    "        \"nl_repsonse\": nl_response\n",
    "    }\n",
    "\n",
    "# runner_viz.py\n",
    "\n",
    "\n",
    "# Lista muy básica de cosas que NO deberían aparecer en el código generado\n",
    "_BANNED_SNIPPETS = [\n",
    "    \"import os\", \"import sys\", \"subprocess\", \"shutil\", \"socket\", \"requests\",\n",
    "    \"urllib\", \"http.server\", \"pickle\", \"dill\", \"cloudpickle\",\n",
    "    \"eval(\", \"exec(\", \"compile(\", \"__import__\", \"input(\", \"open(\", \"Path(\"\n",
    "]\n",
    "\n",
    "def execute_viz_code(py_code: str, timeout_sec: int = 20):\n",
    "    \"\"\"\n",
    "    Ejecuta código Python de visualización (seaborn/matplotlib) en un subproceso aislado.\n",
    "    Espera que el código:\n",
    "      - construya un DataFrame a partir de ROWS embebido\n",
    "      - genere una o dos figuras y las guarde como 'chart1.png' y 'chart2.png'\n",
    "      - haga plt.show() (opcional, no afecta)\n",
    "    Devuelve: dict con ok, charts(base64), stdout, stderr, returncode.\n",
    "    \"\"\"\n",
    "    code = (py_code or \"\").strip()\n",
    "    if code == \"VIS_IMPOSIBLE\":\n",
    "        return {\"ok\": False, \"error\": \"VIS_IMPOSIBLE\"}\n",
    "\n",
    "    # Bloqueo rápido de patrones peligrosos\n",
    "    lowered = code.lower()\n",
    "    for bad in _BANNED_SNIPPETS:\n",
    "        if bad in lowered:\n",
    "            return {\"ok\": False, \"error\": f\"Unsafe code rejected: contains '{bad}'\"}\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as td:\n",
    "        td_path = Path(td)\n",
    "        script_path = td_path / \"viz_script.py\"\n",
    "\n",
    "        # Forzamos backend no interactivo antes de ejecutar el código del modelo\n",
    "        safe_preamble = \"import matplotlib; matplotlib.use('Agg')\\n\"\n",
    "        script_path.write_text(safe_preamble + code, encoding=\"utf-8\")\n",
    "\n",
    "        env = os.environ.copy()\n",
    "        env[\"MPLBACKEND\"] = \"Agg\"\n",
    "\n",
    "        try:\n",
    "            proc = subprocess.run(\n",
    "                [sys.executable, str(script_path)],\n",
    "                cwd=td,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=timeout_sec,\n",
    "                env=env,\n",
    "            )\n",
    "        except subprocess.TimeoutExpired as e:\n",
    "            return {\"ok\": False, \"error\": f\"timeout after {timeout_sec}s\", \"stdout\": e.stdout, \"stderr\": e.stderr}\n",
    "\n",
    "        charts = []\n",
    "        for name in (\"chart1.png\", \"chart2.png\"):\n",
    "            p = td_path / name\n",
    "            if p.exists() and p.is_file():\n",
    "                charts.append({\n",
    "                    \"name\": name,\n",
    "                    \"base64\": base64.b64encode(p.read_bytes()).decode(\"ascii\"),\n",
    "                    \"mime\": \"image/png\",\n",
    "                })\n",
    "\n",
    "        ok = proc.returncode == 0\n",
    "        result = {\n",
    "            \"ok\": ok,\n",
    "            \"returncode\": proc.returncode,\n",
    "            \"stdout\": proc.stdout,\n",
    "            \"stderr\": proc.stderr,\n",
    "            \"charts\": charts,\n",
    "        }\n",
    "        if not ok and not charts:\n",
    "            # Devolvemos algo más explícito si no hay figuras\n",
    "            result.setdefault(\"error\", \"process failed and no charts were produced\")\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "def _detect_mode(request: Request, override: str | None) -> str:\n",
    "    \"\"\"\n",
    "    Devuelve 'html' o 'code'.\n",
    "    Prioridad:\n",
    "      1) override (?format=html|code)\n",
    "      2) Heurística por User-Agent: si parece navegador -> html; si requests/curl -> code\n",
    "      3) Fallback: html\n",
    "    \"\"\"\n",
    "    if override in {\"html\", \"code\"}:\n",
    "        return override\n",
    "    ua = (request.headers.get(\"user-agent\") or \"\").lower()\n",
    "    # Navegadores típicos: chrome/safari/firefox/edge -> HTML\n",
    "    if any(k in ua for k in [\"chrome\", \"safari\", \"firefox\", \"edg\", \"mozilla\"]):\n",
    "        return \"html\"\n",
    "    # Clientes CLI/comunes -> CODE\n",
    "    if \"python-requests\" in ua or \"curl\" in ua or \"httpie\" in ua:\n",
    "        return \"code\"\n",
    "    # Fallback\n",
    "    return \"html\"\n",
    "\n",
    "@app.get(\"/ask-visual\")\n",
    "def ask_visual(\n",
    "    request: Request,\n",
    "    question: str = Query(..., min_length=2),\n",
    "    format: str | None = Query(None, description=\"Opcional: forzar 'html' o 'code'\")\n",
    "):\n",
    "    # -------- 1) NL → SQL --------\n",
    "    query = _ask_gemini(get_config_NL_to_SQL(question))\n",
    "    \n",
    "    if not query:\n",
    "        raise HTTPException(500, detail=\"Gemini no devolvió SQL.\")\n",
    "    \n",
    "    low = query.lower()\n",
    "    \n",
    "    if low.startswith(\"sql_imposible\") or low.startswith(\"no tengo info\"):\n",
    "        # Mismo contrato que tu NL→SQL\n",
    "        mode = _detect_mode(request, format)\n",
    "        if mode == \"code\":\n",
    "            return PlainTextResponse(\"VIS_IMPOSIBLE\", media_type=\"text/x-python; charset=utf-8\", status_code=422)\n",
    "        html = f\"<!doctype html><meta charset='utf-8'><h1>SQL_IMPOSIBLE</h1><p>No hay info suficiente.</p><pre>{query}</pre>\"\n",
    "        return HTMLResponse(html, status_code=422)\n",
    "    \n",
    "    \n",
    "\n",
    "    # -------- 2) Ejecutar SQL (tu Lambda directa) → ROWS --------\n",
    "    event = {\"QUERY\": query}\n",
    "    response = invocar_lambda(\"send_query\", event)\n",
    "    \n",
    "    if response[\"ok\"]:\n",
    "        table_view = response[\"result\"]\n",
    "    elif response[\"ok\"]== \"NOT_INFO\":\n",
    "        table_view = response[\"ok\"]\n",
    "    else:\n",
    "        table_view = \"LAMBDA_ERROR\"\n",
    "\n",
    "    # -------- 3) ROWS → código seaborn --------\n",
    "    code = _ask_gemini(get_config_SQL_GRAPH(question, table_view, query ))\n",
    "    \n",
    "    if not code:\n",
    "        raise HTTPException(500, detail=\"Gemini no devolvió código.\")\n",
    "    mode = _detect_mode(request, format)\n",
    "\n",
    "    # -------- 4) Si piden CODE, devolvemos solo el código --------\n",
    "    if mode == \"code\":\n",
    "        # Puede ser 'VIS_IMPOSIBLE' si el modelo no ve viable la visualización\n",
    "        return PlainTextResponse(code, media_type=\"text/x-python; charset=utf-8\")\n",
    "\n",
    "    # -------- 5) Si piden HTML, ejecutamos el código y embebemos las imágenes --------\n",
    "    run = execute_viz_code(code, timeout_sec=20)\n",
    "    if not run.get(\"ok\"):\n",
    "        html_err = f\"<h1>Error al generar gráficos</h1><pre>{run.get('error') or run.get('stderr') or 'sin detalle'}</pre>\"\n",
    "        return HTMLResponse(html_err, status_code=400)\n",
    "\n",
    "    imgs = \"\".join(\n",
    "        f\"\"\"<figure style=\"max-width:960px\">\n",
    "                <img alt=\"{c['name']}\" src=\"data:image/png;base64,{c['base64']}\" style=\"width:100%;height:auto\"/>\n",
    "                <figcaption style=\"font:14px system-ui;color:#555\">{c['name']}</figcaption>\n",
    "            </figure>\"\"\"\n",
    "        for c in run.get(\"charts\", [])\n",
    "    ) or \"<p>No se generaron gráficos.</p>\"\n",
    "\n",
    "    html = f\"\"\"<!doctype html><meta charset=\"utf-8\"><title>Viz</title>\n",
    "<body style=\"margin:24px;font:16px system-ui\">\n",
    "  <h1 style=\"margin-top:0\">{question}</h1>\n",
    "  <p style=\"color:#666\"><code>{query}</code></p>\n",
    "  {imgs}\n",
    "  <details><summary>STDOUT/STDERR</summary>\n",
    "    <pre>{(run.get('stdout') or '').strip()}</pre>\n",
    "    <pre style=\"color:#a00\">{(run.get('stderr') or '').strip()}</pre>\n",
    "  </details>\n",
    "</body>\"\"\"\n",
    "    return HTMLResponse(html, status_code=200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cf9629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import uvicorn\n",
    "\n",
    "nest_asyncio.apply()  # permite que uvicorn corra dentro de Jupyter\n",
    "\n",
    "uvicorn.run(app, host=\"127.0.0.1\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e30da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "# Crear el DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'cliente': ['Alicia', 'Benito', 'Carlos'],\n",
    "    'ventas': [200, 150, 300]\n",
    "})\n",
    "\n",
    "# Convertir el DataFrame a Parquet en memoria usando BytesIO\n",
    "parquet_buffer = BytesIO()\n",
    "df.to_parquet(parquet_buffer, engine='pyarrow', index=False)\n",
    "\n",
    "object_key = 'in_memory/ventas.parquet'\n",
    "\n",
    "s3.put_object(Bucket=bucket_name, Key=object_key, Body=parquet_buffer.getvalue())\n",
    "print(f\"DataFrame Parquet subido a S3 como '{object_key}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
