{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9b057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Metricas\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import date, datetime\n",
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "\n",
    "\n",
    "import io\n",
    "import boto3\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import io\n",
    "import boto3\n",
    "import requests\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import io\n",
    "from typing import List, Optional\n",
    "import boto3\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "\n",
    "import io\n",
    "from typing import List, Dict\n",
    "import boto3\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import io\n",
    "from typing import List, Dict\n",
    "import boto3\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 3)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28459686",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUKECT_NAME = \"dai03rt-proyecto\"\n",
    "ACCESS_KEY_ID = \"AKIAWZEDMKF3SHFDRH3B\"\n",
    "SECRET_KEY = \"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\"\n",
    "REGION = \"eu-west-1\"\n",
    "ACCESS_TOKEN = \"eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiI2MDkxNGNiOTI5OTI2OGM1ZWQ2NjcxN2UyYmM5MzkyNyIsIm5iZiI6MTc1Njk3NTgxMS41NzMsInN1YiI6IjY4Yjk1MmMzNDYwNDI1OGMwNThjZGM1ZiIsInNjb3BlcyI6WyJhcGlfcmVhZCJdLCJ2ZXJzaW9uIjoxfQ.T-AD7PBLVI5hKAyYiHrheP1xhlGpIBZxJVJq_WlzsGM\"\n",
    "BASE = \"https://api.themoviedb.org/3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955bd7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\",\n",
    "                  aws_access_key_id = ACCESS_KEY_ID,\n",
    "                  aws_secret_access_key = SECRET_KEY,\n",
    "                  region_name=REGION) \n",
    "lambda_ = boto3.client(\"lambda\",\n",
    "                  aws_access_key_id = ACCESS_KEY_ID,\n",
    "                  aws_secret_access_key = SECRET_KEY,\n",
    "                  region_name=REGION) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4919b4f",
   "metadata": {},
   "source": [
    "## TMDB API\n",
    "\n",
    "Para el recopilado de datos usaremos las siguientes enpoints de TMDB:\n",
    "\n",
    "- #### DISCOVER: https://api.themoviedb.org/3/discover/movie\n",
    "    * Esta endpoints nos da una lista de peliculas aleatorias determinando una variedad de filtros\n",
    "    * Nuestro modelo ha sido entrenado con peliculas de hace 20 años, por lo que nuestro fitro debe ser ese rango de fechas\n",
    "    * De esta lista solo nos interesa el **ID** de cada pelicula ya que necesitamos usar otro endpoint para conseguir toda la info necesaria de cada peli\n",
    "    * Toda la información es paginada asi que por ese rango de fechas la request no da la info, solo de la primera pagina, teniendo que hacer una request por cada pagina restante hasta alcanzar el nº max_pages de esa request especifica\n",
    "\n",
    ">**PROBLEMA:**La propia API nos trunquea todas aquellas paginas por encima del numero 500, por lo que nos obliga a que nuestro rango de fechas nunca produzca una request con un numero de paginas superior. Debido a esto hemos tenido que crear unos rangos de fechas especificos, que partimos en request con un step determinado, uno para cada rango. Esto se debe a que la producción de las peliculas es una distribución compleja\n",
    "\n",
    "- #### DETAILS_MOVIES: https://api.themoviedb.org/3/movie/{movie_id}\n",
    "    * Una vez tenemos todos los ids de nuestras peliculas, haremos una peticion a este endpoint por cada id\n",
    "    * Ahora si tenemos toda la info encesaria como, los actores qeu ahn trabajado en la peli, los generos, el presupuesto, la recaudación, etc\n",
    "\n",
    "- #### DETAILS_ACTORS https://api.themoviedb.org/3/person/{person_id}\n",
    "    * Con los ids de los actores recopilados gracias a la anterior url, podemos generar una lista, sin actores repetidos, de los ids de cada actor para buscar sus detalles\n",
    "    * Debido a esto nuestro flujo simpre necesita primero conocer los detalles de las paliculas antes de conocer los detalles de cada actor\n",
    "\n",
    "- #### GENRE_LIST https://api.themoviedb.org/3/genre/movie/list\n",
    "    * En esta enpoint recogemos todos los generos posibles que pueden tener las peliculas\n",
    "    * Cabe recordar que una pelicula puede tener varios generos, y que un genero puede encontrarse en varios peliculas por supuesto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2769a062",
   "metadata": {},
   "source": [
    "## LAMBDAS\n",
    "\n",
    "![Alt](./img/lambdas.png \"Propiedades del BUCKET\")\n",
    "\n",
    "- init_tmdb: funcion que crea en nuestra instancia RDS la base de datos y el usuario que la gestiona\n",
    "- create_tables_tmdb: funcion que crea la estructura de la base de datos\n",
    "- funcion_diaria: funcion de recogida y guardado de nuevos datos cada dia\n",
    "- send_query: funcion para ejecutar queries de tipo DML, en concreto generadoras de vistas\n",
    "- funcion_inicial: funcion de carga incial de peliculas\n",
    "- insert_data_tmdb: se encarga de ejecutar querys DML de tipo insert para rellenar nuestras tablas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0037f76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def invoke_lambda_inicial(payload: dict, async_: bool = False):\n",
    "    \"\"\"Invoca la lambda con el payload dado (sync por defecto).\"\"\"\n",
    "    resp = lambda_.invoke(\n",
    "        FunctionName=\"funcion_inicial\",\n",
    "        InvocationType=\"Event\" if async_ else \"RequestResponse\",\n",
    "        Payload=json.dumps(payload).encode(\"utf-8\"),\n",
    "    )\n",
    "    if async_:\n",
    "        print(\"✔ Invocación async enviada.\")\n",
    "        return None\n",
    "    print(\"HTTP Status:\", resp.get(\"StatusCode\"))\n",
    "    body = resp.get(\"Payload\").read().decode(\"utf-8\")\n",
    "    try:\n",
    "        print(\"Respuesta Lambda:\", json.loads(body))\n",
    "    except Exception:\n",
    "        print(\"Respuesta Lambda (raw):\", body)\n",
    "\n",
    "## Listar funciones Lambda existentes\n",
    "def listar_funciones():\n",
    "    response = lambda_.list_functions()\n",
    "    for f in response['Functions']:\n",
    "        print(f\"{f['FunctionName']} | Runtime: {f['Runtime']} | Última modif: {f['LastModified']}\")\n",
    "        \n",
    "def invocar_lambda(nombre_funcion, payload={}, async_mode=False):\n",
    "    try:\n",
    "        return lambda_.invoke(\n",
    "            FunctionName=nombre_funcion,\n",
    "            InvocationType='Event' if async_mode else 'RequestResponse',\n",
    "            Payload=json.dumps(payload).encode(\"utf-8\"),\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fb978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "listar_funciones()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea4b65d",
   "metadata": {},
   "source": [
    "## SUBIDA DE DATOS A S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6da245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de subida:\n",
    "# funcion lambda: funcion_inicial tambien es una función de subida\n",
    "def save_actor_ids_parquet_to_s3(\n",
    "    actor_ids,\n",
    "    *,\n",
    "    key: str = \"inputs/actors_ids.parquet\",  \n",
    "    compression: str = \"snappy\"              \n",
    ") -> dict:\n",
    "    table = pa.Table.from_pydict({\"actor_id\": [int(x) for x in actor_ids if x is not None]})\n",
    "    buf = io.BytesIO()\n",
    "    pq.write_table(table, buf, compression=compression)\n",
    "    buf.seek(0)\n",
    "\n",
    "    s3.put_object(Bucket=BUKECT_NAME, Key=key, Body=buf.getvalue(), ContentType=\"application/octet-stream\")\n",
    "    return {\"bucket\": BUKECT_NAME, \"key\": key}\n",
    "\n",
    "\n",
    "def save_genres_list_parquet_into_s3(\n",
    "    *,\n",
    "    key: str = \"initial_load/genres/genres.parquet\",\n",
    "    language: str = \"es-ES\",\n",
    "    compression: str = \"gzip\"  # usa \"snappy\" si lo tienes instalado\n",
    ") -> dict:\n",
    "    # 1) Request único a TMDB\n",
    "    url = f\"{BASE}/genre/movie/list\"\n",
    "    r = requests.get(\n",
    "        url,\n",
    "        headers={\"accept\": \"application/json\", \"Authorization\": f\"Bearer {ACCESS_TOKEN}\"},\n",
    "        params={\"language\": language},\n",
    "        timeout=20,\n",
    "    )\n",
    "    r.raise_for_status()\n",
    "    \n",
    "    genres = r.json().get(\"genres\", [])\n",
    "    \n",
    "    if not isinstance(genres, list):\n",
    "        raise ValueError(\"TMDB no devolvió 'genres' como lista\")\n",
    "\n",
    "    # 2) Parquet con el objeto tal cual (lista de dicts con id y name)\n",
    "    table = pa.Table.from_pylist(genres)  # sin tocar los campos\n",
    "    buf = io.BytesIO()\n",
    "    pq.write_table(table, buf, compression=compression)\n",
    "    buf.seek(0)\n",
    "\n",
    "    # 3) Subir a S3\n",
    "    s3.put_object(Bucket=BUKECT_NAME, Key=key, Body=buf.getvalue(), ContentType=\"application/octet-stream\")\n",
    "\n",
    "    return {\"bucket\": BUKECT_NAME, \"key\": key, \"count\": len(genres), \"genres\": genres}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1151d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de bajada de s3\n",
    "# funciones auxiliares para cargar los datos de s3\n",
    "\n",
    "def load_all_actors_from_s3(\n",
    "    prefix: str = \"initial_load/actors/\",\n",
    "\n",
    ") -> List[Dict]:\n",
    "\n",
    "    # 1) Listar todas las keys .parquet (con paginación)\n",
    "    keys: List[str] = []\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    for page in paginator.paginate(Bucket=BUKECT_NAME, Prefix=prefix.rstrip(\"/\") + \"/\"):\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            k = obj[\"Key\"]\n",
    "            if k.endswith(\".parquet\"):\n",
    "                keys.append(k)\n",
    "    keys.sort()\n",
    "\n",
    "    if not keys:\n",
    "        return []\n",
    "\n",
    "    # 2) Leer y acumular todas las filas como dicts\n",
    "    all_rows: List[Dict] = []\n",
    "    for k in keys:\n",
    "        body = s3.get_object(Bucket=BUKECT_NAME, Key=k)[\"Body\"].read()\n",
    "        table = pq.read_table(io.BytesIO(body))\n",
    "        for batch in table.to_batches():\n",
    "            all_rows.extend(batch.to_pylist())\n",
    "\n",
    "    return all_rows\n",
    "\n",
    "def load_all_genres_from_s3():\n",
    "    records: list[dict] = []\n",
    "    body = s3.get_object(Bucket=BUKECT_NAME, Key=\"initial_load/genres/genres.parquet\")[\"Body\"].read()\n",
    "    table = pq.read_table(io.BytesIO(body))\n",
    "    records.extend(table.to_pylist()) \n",
    "    return records\n",
    "\n",
    "def load_all_movies_from_s3(prefix_root: str = \"initial_load/movies/\") -> list[dict]:\n",
    "    prefix = prefix_root.rstrip(\"/\") + \"/\"\n",
    "\n",
    "    # Listar todas las claves .parquet bajo el prefijo\n",
    "    keys = []\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    for page in paginator.paginate(Bucket=BUKECT_NAME, Prefix=prefix):\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            k = obj[\"Key\"]\n",
    "            if k.endswith(\".parquet\"):\n",
    "                keys.append(k)\n",
    "\n",
    "    keys.sort()  # lectura determinista (opcional)\n",
    "\n",
    "    # Cargar y acumular\n",
    "    records: list[dict] = []\n",
    "    for key in keys:\n",
    "        body = s3.get_object(Bucket=BUKECT_NAME, Key=key)[\"Body\"].read()\n",
    "        table = pq.read_table(io.BytesIO(body))\n",
    "        records.extend(table.to_pylist())  # lista de dicts por fila\n",
    "\n",
    "    return records\n",
    "\n",
    "def get_actors_id_from_list_long(rows: List[Dict[str, Any]], credits_col: str = \"credits\") -> List[int]:\n",
    "    \"\"\"\n",
    "    Extrae IDs únicos de actores desde una lista de diccionarios (filas).\n",
    "    Cada fila debe tener la clave 'credits' con un dict que contenga 'cast'.\n",
    "    \"\"\"\n",
    "    all_ids = []\n",
    "    for row in rows:\n",
    "        entry = row.get(credits_col)\n",
    "        if isinstance(entry, dict) and \"cast\" in entry:\n",
    "            for actor in entry[\"cast\"]:\n",
    "                if isinstance(actor, dict) and \"id\" in actor:\n",
    "                    all_ids.append(actor[\"id\"])\n",
    "\n",
    "    # Elimina duplicados preservando orden\n",
    "    seen = set()\n",
    "    unique_ids = []\n",
    "    for actor_id in all_ids:\n",
    "        if actor_id not in seen:\n",
    "            seen.add(actor_id)\n",
    "            unique_ids.append(actor_id)\n",
    "\n",
    "    return unique_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1633b07",
   "metadata": {},
   "source": [
    "## 1. Subimos los datos de las todas las peliculas de hace 20 años:\n",
    "\n",
    "#### funcion_inicial\n",
    "\n",
    "Se encarga de conseguir todos los ids de las peliculas en un rango de fechas\n",
    "- Por ejemplo las fechas 2005, 1, 1 hasta 2010, 12, 31\n",
    "- Este rango se partirá a su vez en en rangos de fechas mas pequeños con la longitud marcada por el step, en este caso 182 días.\n",
    "- Estos nuevos rangos los lllamamos ventantas y represenan la agrupacion de fechas entre 2005, 1, 1 hasta 2010, 12, 31, pero con una longitud de 182 (a excepcion de la última que no suele coincidir un step perfecto)\n",
    "- En cada ventana tenemos un nuemor de paginas totales\n",
    "- En cada pagina tenemos 20 peliculas\n",
    "- Cuando tenemos todos los ids de todas las paginas de una ventana, se hara un request a details de movies y cuando hayamos conseguido los detalles de todas las peliculas de todas las paginas de esa ventana en concreto se guardara un archvio en s3 con los detalles de las (20xN_paginas_totales) peliculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906c1e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rangos_step = [\n",
    "    (date(2005, 1, 1),  date(2010, 12, 31), 182),\n",
    "    (date(2011, 1, 1),  date(2013, 9, 16),  120),\n",
    "    (date(2013, 9, 17), date(2014, 1, 6),   100),\n",
    "    (date(2014, 1, 7),  date(2015, 12, 31), 110),\n",
    "    (date(2016, 1, 1),  date(2021, 12, 31), 70),\n",
    "    (date(2022, 1, 1),  date(2024, 10, 16), 60),\n",
    "    (date(2024, 10, 17),date(2024, 12, 16), 30),\n",
    "    (date(2024, 12, 17),date.today(),       60),\n",
    "]\n",
    "\n",
    "date_range_n = 0\n",
    "for f_incial, f_final, step in rangos_step:\n",
    "    date_range_n += 1\n",
    "    f_from = f_incial.strftime(\"%Y-%m-%d\")\n",
    "    f_to = f_final.strftime(\"%Y-%m-%d\")\n",
    "    payload = {\n",
    "        \"from\": f_from,\n",
    "        \"to\":   f_to,\n",
    "        \"step_days\": step,\n",
    "        \"date_range_idx\": date_range_n\n",
    "    }\n",
    "    invoke_lambda_inicial(payload, async_=True)\n",
    "    \n",
    "# Datos de las peliculas subidos ahora en s3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d61af20",
   "metadata": {},
   "source": [
    "## 2. Subimos los ids de los actores\n",
    "- Necesitmaos subir los ids de todos lso actores a s3, para despues subir todos los detalles de los actores con esos ids. \n",
    "- Los ids se guardan en ***input/actors_ids.parquet***\n",
    "- Con la lista de ids en s3 ejecutamos \n",
    "\n",
    "\n",
    ">NOTA: es necesario que la lista de ids este en s3 y no en memoria por ejemplo para que la lambda no se corte por tardar mucho en su ejecución\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5400d295",
   "metadata": {},
   "outputs": [],
   "source": [
    "actors_ids = get_actors_id_from_list_long(load_all_movies_from_s3())\n",
    "len(actors_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f83e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Guardar la lista en S3\n",
    "ref = save_actor_ids_parquet_to_s3(\n",
    "    actors_ids,\n",
    "    key=\"inputs/actors_ids.parquet\"           # <- esta ruta es la que usará la Lambda\n",
    ")\n",
    "ref\n",
    "\n",
    "# Los ids de actores que salen en nuestras peliculas se han guardado correctamente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085c9e8e",
   "metadata": {},
   "source": [
    "## 3. Subimos la data (detalles) de los actores\n",
    "\n",
    "#### load_actors_into_s3\n",
    "\n",
    "- Esta función recibe una lista de ids de actores\n",
    "- Hace una peticion a la API tmbd para conseguir los detalles de cada actor diferente\n",
    "- Guarda la informacion en uno o varios documentos en ***intial_load/actors/job_20250922/X_actors.parquet***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e515dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Invocar la Lambda para que lea esos IDs y procese\n",
    "payload = {\n",
    "    \"ids_s3\":ref,\n",
    "    \"start_at\": 0,\n",
    "    \"part_index\": 1,\n",
    "    \"s3_prefix\": \"initial_load/actors/job_20250922\",\n",
    "    \"return_saved_keys\": True\n",
    "}\n",
    "\n",
    "invocar_lambda(nombre_funcion=\"load_actors_into_s3\", payload=payload, async_mode=True)\n",
    "\n",
    "# La data en detalle de cada uno de los actores esta en s3 ahora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d919b2d6",
   "metadata": {},
   "source": [
    "## 4. Subimos la pequeña lista de genres\n",
    "- Como se trata de una request unicamente, no es necesario una lambda\n",
    "- La lista que nos devuevle la request es muy pequeña asi que la podemos subir a s3 en la misma funcion\n",
    "- Se guardara en ***initial_load/genres/genres.parquet***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d877c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = save_genres_list_parquet_into_s3()\n",
    "resp\n",
    "# Ahora tenemos los datos de la lista de géneros de películas en s3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3daeda0",
   "metadata": {},
   "source": [
    "# ESTRUCTURA DE s3\n",
    "\n",
    "### s3://dai03rt-proyecto\n",
    "\n",
    "\n",
    "<pre style=\"font-size:12px; line-height:1.25; white-space:pre;\">\n",
    "├── inputs/\n",
    "│   └── actors_ids.parquet\n",
    "│       └── Lista de IDs de actores (Parquet, col: actor_id).\n",
    "│          Fuente: subida manual desde local para alimentar la Lambda de actores.\n",
    "│\n",
    "└── initial_load/\n",
    "    ├── movies/\n",
    "    │   └── date_range_<N>/\n",
    "    │       └── ventana_<N>/\n",
    "    │           └── <part_index>_movies.parquet\n",
    "    │               └── Detalles de películas (discover + details) por rangos de fechas/ventanas.\n",
    "    │                  Fuente: Lambda de “discover movie + details” con paginación y auto-resume.\n",
    "    │\n",
    "    ├── actors/\n",
    "    │   ├── <part_index>_actors.parquet            (cuando no se usa s3_prefix “job_*”)\n",
    "    │   └── job_YYYYMMDD/\n",
    "    │       └── <part_index>_actors.parquet\n",
    "    │           └── Detalles de personas (/person/{id}) descargados por lotes.\n",
    "    │              Fuente: Lambda `load_actors_into_s3` leyendo IDs (de payload o de S3).\n",
    "    │              Particionado por “part_index”; puede haber múltiples ficheros por reintentos/timebox.\n",
    "    │\n",
    "    └── genres/\n",
    "        └── genres.parquet\n",
    "            └── Catálogo de géneros de TMDB (/genre/movie/list), ~20 filas, cols: id, name.\n",
    "            └── Fuente: script local que hace 1 request y sube el Parquet.\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f152b354",
   "metadata": {},
   "source": [
    "## CARGA DE DATOS desde S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cd5c0e",
   "metadata": {},
   "source": [
    "Conseguidas todas las peliculas, rescatamos los actores por pelicula para llamar a la función save_all_actors con la lista de ids de atores construida a partir de toda la info de películas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273466a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENRES_DATA = load_all_genres_from_s3()\n",
    "GENRES_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff2976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOVIES_DATA = load_all_movies_from_s3()\n",
    "MOVIES_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e03513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTORS_DATA = load_all_actors_from_s3()\n",
    "ACTORS_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbc0b33",
   "metadata": {},
   "source": [
    "## RDS: Preparación de la instancia RDS\n",
    "\n",
    "![Alt](./img/rds.png \"Propiedades del RDS\")\n",
    "\n",
    "Antes de empezar a usar nuestra instancia de RDS necesitamos:\n",
    "- Crear la base de datos y un usuario que la vaya gestionar\n",
    "- Crear la estructura de las tablas y sus relaciones\n",
    "- Insertar todos los datos inciales (Guardados en s3 __initial_load/*/__) \n",
    "- Insertar o modificar los nuevos datos (Guardados en dayly/movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942ad484",
   "metadata": {},
   "source": [
    "### RDS.1 Primero inicializamos nuestra base de datos\n",
    "\n",
    "DOCUMENTACION: **./lambdas/init_tmdb**\n",
    "\n",
    "Para ello llamamos a la LAMBDA __\"init_tmdb\"__ que hace lo siguinte:\n",
    "\n",
    "- Creamos una base de datos \"tmdb\" \n",
    "- Creamos un usuario \"user\" que será el que maneje las tablas\n",
    "    * Este usuario **NO** podrá eliminar la base de datos por seguridad\n",
    "    * Podrá crear nuevas tablas y borrarlas pero nunca la base de datos entera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f95a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = invocar_lambda(nombre_funcion=\"init_tmdb\")\n",
    "json.loads(resp.get(\"Payload\").read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9477ef0d",
   "metadata": {},
   "source": [
    "### RDS.2 Creamos la estrucutura de nuestra base de datos:\n",
    "\n",
    "DOCUMENTACION: **./lambdas/create_tables_tmdb**\n",
    "\n",
    "![Alt](./img/tmdb_schema.webp \"Propiedades del RDS\")\n",
    "\n",
    "Llmamaos a la LAMBDA __\"create_tables_tmdb\"__. Creación de las tablas y de sus relaciones, la lambda ejecuta un archivo .sql guardado en el propio codigo de la lambda.\n",
    "\n",
    "Este es es archivo: \n",
    "\n",
    "```sql\n",
    "-- SCHEMA: tmdb (tablas en 'public')\n",
    "\n",
    "-- TABLE: movies\n",
    "CREATE TABLE IF NOT EXISTS public.movies (\n",
    "    id              INTEGER PRIMARY KEY,                      -- TMDB movie id (manual)\n",
    "    title           TEXT NOT NULL,\n",
    "    popularity      REAL,                                     -- float4\n",
    "    vote_average    NUMERIC(3,1) \n",
    "    CHECK (vote_average >= 0 AND vote_average <= 10),     -- 0.0..10.0\n",
    "    runtime         SMALLINT CHECK (runtime >= 0),            -- minutos\n",
    "    budget          BIGINT CHECK (budget  >= 0),              -- entero (unidades monetarias)\n",
    "    revenue         BIGINT CHECK (revenue >= 0),\n",
    "    overview        TEXT,\n",
    "    release_date    DATE,\n",
    "    success         BOOLEAN\n",
    ");\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS idx_movies_release_date ON public.movies (release_date);\n",
    "CREATE INDEX IF NOT EXISTS idx_movies_popularity  ON public.movies (popularity);\n",
    "CREATE INDEX IF NOT EXISTS idx_movies_title_lower ON public.movies (lower(title));\n",
    "\n",
    "-- TABLE: actors (cast)\n",
    "CREATE TABLE IF NOT EXISTS public.actors (\n",
    "    id          INTEGER PRIMARY KEY,                          \n",
    "    name        TEXT NOT NULL,\n",
    "    age         SMALLINT CHECK (age IS NULL OR (age BETWEEN 0 AND 150)),\n",
    "    gender      TEXT CHECK (gender IN ('Not set', 'Female', 'Male', 'Non-binary')),\n",
    "    popularity  REAL\n",
    ");\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS idx_actors_name_lower ON public.actors (lower(name));\n",
    "CREATE INDEX IF NOT EXISTS idx_actors_popularity ON public.actors (popularity);\n",
    "\n",
    "\n",
    "-- TABLE: genres\n",
    "CREATE TABLE IF NOT EXISTS public.genres (\n",
    "    id      INTEGER PRIMARY KEY,          -- TMDB genre id (manual)\n",
    "    name    TEXT NOT NULL UNIQUE\n",
    ");\n",
    "\n",
    "-- TABLE: movie_actors  (credits → cast, relación N:M)\n",
    "CREATE TABLE IF NOT EXISTS public.movie_actors (\n",
    "    movie_id INTEGER NOT NULL REFERENCES public.movies(id) ON DELETE CASCADE,\n",
    "    actor_id INTEGER NOT NULL REFERENCES public.actors(id) ON DELETE CASCADE,\n",
    "    PRIMARY KEY (movie_id, actor_id)\n",
    ");\n",
    "\n",
    "-- TABLE: movie_genres  (genres, relación N:M)\n",
    "CREATE TABLE IF NOT EXISTS public.movie_genres (\n",
    "    movie_id INTEGER NOT NULL REFERENCES public.movies(id) ON DELETE CASCADE,\n",
    "    genre_id INTEGER NOT NULL REFERENCES public.genres(id) ON DELETE CASCADE,\n",
    "    PRIMARY KEY (movie_id, genre_id)\n",
    ");\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS idx_movie_actors_actor ON public.movie_actors (actor_id);\n",
    "CREATE INDEX IF NOT EXISTS idx_movie_genres_genre ON public.movie_genres (genre_id);\n",
    "```\n",
    "\n",
    ">**Nota:** Importante el orden de creación de las tablas, debido a la definción de sus relaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019fd0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = invocar_lambda(nombre_funcion=\"create_tables_tmdb\")\n",
    "tables = json.loads(resp.get(\"Payload\").read().decode(\"utf-8\"))[\"structure\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c48d72",
   "metadata": {},
   "source": [
    "Revisamos que las estrcuturas creadas han sido las correctas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5690e3",
   "metadata": {},
   "source": [
    "Para la tabla __actors__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bca7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tables[\"actors\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972721a6",
   "metadata": {},
   "source": [
    "Para la tabla __movies__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3330d89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tables[\"movies\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139e5581",
   "metadata": {},
   "source": [
    "Para la tabla __movies__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32522def",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tables[\"genres\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89425ce9",
   "metadata": {},
   "source": [
    "Para la tabla __movie_actors__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8089484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tables[\"movie_actors\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24371248",
   "metadata": {},
   "source": [
    "Para la tabla __movie_genres__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37348870",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tables[\"movie_genres\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86f5494",
   "metadata": {},
   "source": [
    "### RDS.3 Insertamos los datos guardados en S3\n",
    "\n",
    "DOCUMENTACION: **./lambdas/insert_data_tmdb**\n",
    "\n",
    "Por lo tanto debemos hacer una limpieza de datos guardados en S3, para insertarlos en nuestras tablas SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f010fda",
   "metadata": {},
   "source": [
    ">##### Funciones de limpieza S3 -> limpeza -> RDS (sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224b2a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_success_column(df):\n",
    "    df_original = df.copy()\n",
    "    if 'revenue' in df.columns and 'budget' in df.columns:\n",
    "        df[\"success\"] = (df_original[\"revenue\"] > df_original[\"budget\"]) & (df_original[\"vote_average\"] > 7)\n",
    "    else:\n",
    "        raise ValueError(\"El DataFrame debe contener las columnas 'revenue' y 'budget'\")\n",
    "    return df\n",
    "\n",
    "def movies_sql(df):\n",
    "    df_movies_original = df.copy()\n",
    "    df = df_movies_original[[\"id\", \"title\",\"popularity\", \"vote_average\", \"runtime\", \"budget\", \"revenue\", \"overview\", \"genres\", \"credits\", \"release_date\"]]\n",
    "    df[\"credits\"] = [_get_values_for_row(credits[\"cast\"], \"id\") for credits in df_movies_original[\"credits\"]]\n",
    "    df[\"genres\"] = [_get_values_for_row(genres, \"id\") for genres in df_movies_original[\"genres\"]]\n",
    "    df = _add_success_column(df)\n",
    "    #df = impute_with_median(df)\n",
    "    return df\n",
    "\n",
    "def prepare_table_for_ia(df):\n",
    "    return  df[[\"popularity\",\"vote_average\",\"runtime\",\"budget\", \"succes\"]]\n",
    "\n",
    "# Para las columnas de CREDITS y GENRES donde nos vienen la información que relaciona la\n",
    "# tabla películas con actores y generos\n",
    "def _get_values_for_row(items, column_name):\n",
    "    return [item.get(column_name) for item in items if isinstance(item, dict) and column_name in item]\n",
    "\n",
    "def movie_genres_sql(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_exploded = df[[\"id\", \"genres\"]].copy()\n",
    "    return (\n",
    "        df_exploded\n",
    "        .explode(\"genres\")\n",
    "        .dropna(subset=[\"genres\"])\n",
    "        .rename(columns={\"id\": \"movie_id\", \"genres\": \"genre_id\"})\n",
    "        .drop_duplicates(ignore_index=True)\n",
    "        .reset_index(drop=True)\n",
    "        .astype({\"movie_id\": \"int64\", \"genre_id\": \"int64\"})\n",
    "    )\n",
    "def movie_actors_sql(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_exploded = df[[\"id\", \"credits\"]].copy()\n",
    "    return (\n",
    "        df_exploded\n",
    "        .explode(\"credits\")                          # cada actor en fila propia\n",
    "        .dropna(subset=[\"credits\"])                  # elimina None/NaN\n",
    "        .rename(columns={\"id\": \"movie_id\", \"credits\": \"actor_id\"})\n",
    "        .astype({\"movie_id\": \"int64\", \"actor_id\": \"int64\"})\n",
    "        .drop_duplicates(ignore_index=True)          # clave primaria (movie_id, actor_id)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "def compute_actor_age(birthday: str | None, deathday: str | None) -> int | None:\n",
    "    if not birthday or pd.isna(birthday) or birthday == \"None\":\n",
    "        return None   # no se conoce la edad\n",
    "\n",
    "    try:\n",
    "        birth_date = datetime.strptime(str(birthday)[:10], \"%Y-%m-%d\").date()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    if deathday and deathday != \"None\" and not pd.isna(deathday):\n",
    "        try:\n",
    "            end_date = datetime.strptime(str(deathday)[:10], \"%Y-%m-%d\").date()\n",
    "        except Exception:\n",
    "            end_date = date.today()\n",
    "    else:\n",
    "        end_date = date.today()\n",
    "\n",
    "    age = end_date.year - birth_date.year - (\n",
    "        (end_date.month, end_date.day) < (birth_date.month, birth_date.day)\n",
    "    )\n",
    "\n",
    "    return age if age >= 0 else None\n",
    "\n",
    "def map_actor_gender(df: pd.DataFrame, gender_col: str = \"gender\") -> pd.DataFrame:\n",
    "    \n",
    "    mapping = {\n",
    "        0: \"Not set\",\n",
    "        1: \"Female\",\n",
    "        2: \"Male\",\n",
    "        3: \"Non-binary\"\n",
    "    }\n",
    "    df = df.copy()\n",
    "    df[gender_col] = df[gender_col].replace(mapping)\n",
    "    return df\n",
    "\n",
    "def clean_actors(df: pd.DataFrame):\n",
    "    toret = df[[\"id\",\"name\",  \"birthday\", \"deathday\", \"gender\", \"popularity\"]]\n",
    "    toret[\"age\"] = df.apply(\n",
    "        lambda r: compute_actor_age(r[\"birthday\"], r[\"deathday\"]),\n",
    "        axis=1\n",
    "    )\n",
    "    toret = toret.drop(columns=[\"birthday\", \"deathday\"])\n",
    "    toret = map_actor_gender(toret)\n",
    "    return toret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cdd7b4",
   "metadata": {},
   "source": [
    "#### Primero de todo vemos como es estructura de los datos cargados desde S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b033ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies_s3 = pd.DataFrame(MOVIES_DATA)\n",
    "df_movies_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c464a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actors_s3 = pd.DataFrame(ACTORS_DATA)\n",
    "df_actors_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4013e410",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genres_s3 = pd.DataFrame(GENRES_DATA)\n",
    "df_genres_s3\n",
    "\n",
    "# !Esta perfecto no hay que hacerle nada, podemos guardar sus datos asi en RDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1a640f",
   "metadata": {},
   "source": [
    "#### Preparamos ***df_movies***\n",
    "A partir del data frame que generamos con la info que se enceuntra en s3 de las peliculas, generamos un dataframe limpio con la misma estructura que las tablas de sql. \n",
    "\n",
    "- Transormamos las columnas de credit y de genres a una lista de ids unicamente\n",
    "- Nos quedamos con las columnas de la que necesitamos para la base  de datos\n",
    "- Le añadimos la columna succes que será nuestro feature a predecir más adelante\n",
    ">NOTA: Las columnas de **genres** y **credits** que aun las necesitamos para saber las relaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3d35d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = movies_sql(df_movies_s3)\n",
    "df_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e180c6d8",
   "metadata": {},
   "source": [
    "#### Preparamos ***df_movie_genres***\n",
    "A partir de la columna credist ya formateada con los ids de los actores solo conseguimos la tabla relacional de peliculas actores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5b59e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movie_genres = movie_genres_sql(df_movies)\n",
    "df_movie_genres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a04534c",
   "metadata": {},
   "source": [
    "#### Preparamos ***df_movie_actors***\n",
    "A partir de la columna credist ya formateada con los ids de los actores solo conseguimos la tabla relacional de peliculas actores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd703526",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movie_actors = movie_actors_sql(df_movies)\n",
    "df_movie_actors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c68db9",
   "metadata": {},
   "source": [
    "#### Preparamos ***df_actors***\n",
    "\n",
    "- Transormamos genre de numero a texto\n",
    "- Calculamos la edad apartir del día de nacimiento y del día de la muerte (None indica que sigue vivo)\n",
    "- Nos quedamos con las columnas necesarias para la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10992c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actors = clean_actors(df_actors_s3)\n",
    "df_actors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2f1d1f",
   "metadata": {},
   "source": [
    "#### Preparamos GENRES_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf424313",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genres = pd.DataFrame(GENRES_DATA)\n",
    "df_genres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9363e4",
   "metadata": {},
   "source": [
    "#### Funciones auxiliares (DF pandas -> INSERT sql)\n",
    "\n",
    "Estas funcionaes nos transforman los data frames que hemos construido, en un insert con sus propios datos para la tabla en la base de datos RDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5c571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_movies_insert(df: pd.DataFrame, table: str = \"public.movies\") -> str:\n",
    "    required = [\n",
    "        \"id\", \"title\", \"popularity\", \"vote_average\", \"runtime\",\n",
    "        \"budget\", \"revenue\", \"overview\", \"release_date\", \"success\"\n",
    "    ]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required column(s): {missing}\")\n",
    "\n",
    "    def esc(s: str) -> str:\n",
    "        # Escape single quotes for SQL strings\n",
    "        return s.replace(\"'\", \"''\")\n",
    "\n",
    "    def fmt_str(s) -> str:\n",
    "        if pd.isna(s) or s is None:\n",
    "            return \"''\"\n",
    "        return f\"'{esc(str(s))}'\"\n",
    "\n",
    "    def fmt_int(x) -> str:\n",
    "        if pd.isna(x) or x is None:\n",
    "            return \"NULL\"\n",
    "        return str(int(x))\n",
    "\n",
    "    def fmt_dec(x, decimals: int) -> str:\n",
    "        # Use Decimal to avoid float artifacts and force fixed decimals\n",
    "        if pd.isna(x) or x is None:\n",
    "            return \"NULL\"\n",
    "        q = Decimal(\"1\").scaleb(-decimals)  # e.g., decimals=4 -> Decimal('0.0001')\n",
    "        val = Decimal(str(x)).quantize(q, rounding=ROUND_HALF_UP)\n",
    "        return f\"{val:.{decimals}f}\"\n",
    "\n",
    "    def fmt_date(d) -> str:\n",
    "        if pd.isna(d) or d is None:\n",
    "            return \"NULL\"\n",
    "        if isinstance(d, (datetime, date)):\n",
    "            return f\"DATE '{d.strftime('%Y-%m-%d')}'\"\n",
    "        s = str(d).strip()\n",
    "        if not s:\n",
    "            return \"NULL\"\n",
    "        try:\n",
    "            ymd = s[:10]\n",
    "            datetime.strptime(ymd, \"%Y-%m-%d\")\n",
    "            return f\"DATE '{ymd}'\"\n",
    "        except Exception:\n",
    "            return \"NULL\"\n",
    "\n",
    "    def fmt_bool(b) -> str:\n",
    "        if pd.isna(b) or b is None:\n",
    "            return \"FALSE\"\n",
    "        return \"TRUE\" if bool(b) else \"FALSE\"\n",
    "\n",
    "    values = []\n",
    "    for _, r in df.iterrows():\n",
    "        row_sql = (\n",
    "            fmt_int(r[\"id\"]),                    # id\n",
    "            fmt_str(r[\"title\"]),                 # title\n",
    "            fmt_dec(r[\"popularity\"], 4),         # popularity\n",
    "            fmt_dec(r[\"vote_average\"], 1),       # vote_average\n",
    "            fmt_int(r[\"runtime\"]),               # runtime\n",
    "            fmt_int(r[\"budget\"]),                # budget\n",
    "            fmt_int(r[\"revenue\"]),               # revenue\n",
    "            fmt_str(r[\"overview\"] if \"overview\" in r else \"\"),  # overview\n",
    "            fmt_date(r[\"release_date\"]),         # release_date\n",
    "            fmt_bool(r[\"success\"])               # success\n",
    "        )\n",
    "        values.append(f\"({', '.join(row_sql)})\")\n",
    "\n",
    "    if not values:\n",
    "        raise ValueError(\"DataFrame is empty; no rows to insert.\")\n",
    "\n",
    "    head = (\n",
    "        f\"INSERT INTO {table} (\\n\"\n",
    "        f\"  id, title, popularity, vote_average, runtime, budget, revenue, overview, release_date, success\\n\"\n",
    "        f\") VALUES\\n\"\n",
    "    )\n",
    "    tail = \"\\nON CONFLICT (id) DO NOTHING;\"\n",
    "    return head + \",\\n\".join(values) + tail\n",
    "\n",
    "def df_to_movie_genres_insert(df: pd.DataFrame, table: str = \"public.movie_genres\") -> str:\n",
    "    # Validate columns\n",
    "    required = {\"movie_id\", \"genre_id\"}\n",
    "    if not required.issubset(df.columns):\n",
    "        raise ValueError(f\"DataFrame must contain columns: {sorted(required)}\")\n",
    "\n",
    "    # Remove rows with nulls and exact duplicates (sane pre-checks for PK (movie_id, genre_id))\n",
    "    df_clean = (\n",
    "        df.loc[:, [\"movie_id\", \"genre_id\"]]\n",
    "          .dropna(subset=[\"movie_id\", \"genre_id\"])\n",
    "          .drop_duplicates(ignore_index=True)\n",
    "    )\n",
    "\n",
    "    if df_clean.empty:\n",
    "        raise ValueError(\"DataFrame is empty after cleaning; no rows to insert.\")\n",
    "\n",
    "    # Build VALUES tuples\n",
    "    values = []\n",
    "    for _, r in df_clean.iterrows():\n",
    "        try:\n",
    "            movie_id = int(r[\"movie_id\"])\n",
    "            genre_id = int(r[\"genre_id\"])\n",
    "        except Exception:\n",
    "            raise ValueError(f\"Non-integer IDs found in row: {r.to_dict()}\")\n",
    "        values.append(f\"({movie_id}, {genre_id})\")\n",
    "\n",
    "    head = (\n",
    "        f\"INSERT INTO {table} (\\n\"\n",
    "        f\"  movie_id, genre_id\\n\"\n",
    "        f\") VALUES\\n\"\n",
    "    )\n",
    "    tail = \"\\nON CONFLICT DO NOTHING;\"\n",
    "\n",
    "    return head + \",\\n\".join(values) + tail\n",
    "\n",
    "def df_to_actors_insert(\n",
    "    df: pd.DataFrame,\n",
    "    table: str = \"public.actors\",\n",
    "    validate_gender: bool = True,\n",
    ") -> str:\n",
    "    required = {\"id\", \"name\", \"age\", \"gender\", \"popularity\"}\n",
    "    if not required.issubset(df.columns):\n",
    "        raise ValueError(f\"DataFrame must contain columns: {sorted(required)}\")\n",
    "\n",
    "    allowed_genders = {\"Not set\", \"Female\", \"Male\", \"Non-binary\"}\n",
    "\n",
    "    def esc(s: str) -> str:\n",
    "        return s.replace(\"'\", \"''\")\n",
    "\n",
    "    def fmt_str(s) -> str:\n",
    "        if pd.isna(s) or s is None:\n",
    "            return \"''\"\n",
    "        return f\"'{esc(str(s))}'\"\n",
    "\n",
    "    def fmt_int(x) -> str:\n",
    "        if pd.isna(x) or x is None:\n",
    "            return \"NULL\"\n",
    "        return str(int(x))\n",
    "\n",
    "    def fmt_float(x, decimals: int = 4) -> str:\n",
    "        if pd.isna(x) or x is None:\n",
    "            return \"NULL\"\n",
    "        return f\"{float(x):.{decimals}f}\"\n",
    "\n",
    "    def fmt_gender(g) -> str:\n",
    "        if pd.isna(g) or g is None:\n",
    "            # si falta, lo tratamos como 'Not set'\n",
    "            g = \"Not set\"\n",
    "        g_str = str(g)\n",
    "        if validate_gender and g_str not in allowed_genders:\n",
    "            raise ValueError(\n",
    "                f\"Invalid gender value '{g_str}'. Expected one of {sorted(allowed_genders)}.\"\n",
    "            )\n",
    "        return f\"'{esc(g_str)}'\"\n",
    "\n",
    "    values = []\n",
    "    for _, r in df.iterrows():\n",
    "        tup = (\n",
    "            fmt_int(r[\"id\"]),            # id\n",
    "            fmt_str(r[\"name\"]),          # name\n",
    "            fmt_int(r[\"age\"]),           # age -> NULL si None/NaN\n",
    "            fmt_gender(r[\"gender\"]),     # gender como TEXT validado\n",
    "            fmt_float(r[\"popularity\"], 4)# popularity\n",
    "        )\n",
    "        values.append(f\"({', '.join(tup)})\")\n",
    "\n",
    "    if not values:\n",
    "        raise ValueError(\"DataFrame is empty; no rows to insert.\")\n",
    "\n",
    "    head = (\n",
    "        f\"INSERT INTO {table} (\\n\"\n",
    "        f\"  id, name, age, gender, popularity\\n\"\n",
    "        f\") VALUES\\n\"\n",
    "    )\n",
    "    tail = \"\\nON CONFLICT (id) DO NOTHING;\"\n",
    "\n",
    "    return head + \",\\n\".join(values) + tail\n",
    "\n",
    "def df_to_movie_actors_insert(df: pd.DataFrame, table: str = \"public.movie_actors\") -> str:\n",
    "    required = {\"movie_id\", \"actor_id\"}\n",
    "    if not required.issubset(df.columns):\n",
    "        raise ValueError(f\"DataFrame must contain columns: {sorted(required)}\")\n",
    "\n",
    "    # Clean duplicates and NaNs\n",
    "    df_clean = (\n",
    "        df.loc[:, [\"movie_id\", \"actor_id\"]]\n",
    "          .dropna(subset=[\"movie_id\", \"actor_id\"])\n",
    "          .drop_duplicates(ignore_index=True)\n",
    "    )\n",
    "\n",
    "    if df_clean.empty:\n",
    "        raise ValueError(\"DataFrame is empty after cleaning; no rows to insert.\")\n",
    "\n",
    "    values = []\n",
    "    for _, r in df_clean.iterrows():\n",
    "        movie_id = int(r[\"movie_id\"])\n",
    "        actor_id = int(r[\"actor_id\"])\n",
    "        values.append(f\"({movie_id}, {actor_id})\")\n",
    "\n",
    "    head = (\n",
    "        f\"INSERT INTO {table} (\\n\"\n",
    "        f\"  movie_id, actor_id\\n\"\n",
    "        f\") VALUES\\n\"\n",
    "    )\n",
    "    tail = \"\\nON CONFLICT DO NOTHING;\"\n",
    "\n",
    "    return head + \",\\n\".join(values) + tail\n",
    "\n",
    "def df_to_genres_insert(df: pd.DataFrame, table: str = \"public.genres\") -> str:\n",
    "    required = {\"id\", \"name\"}\n",
    "    if not required.issubset(df.columns):\n",
    "        raise ValueError(f\"DataFrame must contain columns: {sorted(required)}\")\n",
    "\n",
    "    def esc(s: str) -> str:\n",
    "        return s.replace(\"'\", \"''\")\n",
    "\n",
    "    def fmt_str(s) -> str:\n",
    "        if pd.isna(s) or s is None:\n",
    "            return \"''\"\n",
    "        return f\"'{esc(str(s))}'\"\n",
    "\n",
    "    def fmt_int(x) -> str:\n",
    "        if pd.isna(x) or x is None:\n",
    "            return \"NULL\"\n",
    "        return str(int(x))\n",
    "\n",
    "    values = []\n",
    "    for _, r in df.iterrows():\n",
    "        tup = (\n",
    "            fmt_int(r[\"id\"]),       # id\n",
    "            fmt_str(r[\"name\"]),     # name\n",
    "        )\n",
    "        values.append(f\"({', '.join(tup)})\")\n",
    "\n",
    "    if not values:\n",
    "        raise ValueError(\"DataFrame is empty; no rows to insert.\")\n",
    "\n",
    "    head = f\"INSERT INTO {table} (\\n  id, name\\n) VALUES\\n\"\n",
    "    tail = \"\\nON CONFLICT (id) DO NOTHING;\"\n",
    "\n",
    "    return head + \",\\n\".join(values) + tail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a6f7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_genres = df_to_genres_insert(df_genres)\n",
    "print(insert_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf541eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "invocar_lambda(\"insert_data_tmdb\",{\"QUERY\": insert_genres})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d22061",
   "metadata": {},
   "source": [
    "# MODELO Clasificador - ÉXITO / NO ÉXITO\n",
    "\n",
    "- Regresión Logística\n",
    "- Metrica de éxito: 1 if revenue > budget and vote_average >= 7 else 0 \n",
    "- Solo usaremos datos de la tabla movies\n",
    "- Datos de entrenamiento:\n",
    "    * ***budget***: lo que ha costado producir la película\n",
    "    * ***popularity***: lo conocida que ha sido la película, en una medida expresada como float\n",
    "    * ***runtime***: la duración ne  min de la película\n",
    "    * ***vote_average***: la votación media que obtuvo la película\n",
    "- En el procesado de datos al solo usar columnas numéricas tendremos que escalar los datos y rellenar los nulls con la mediana\n",
    "\n",
    "Nuestros datos ahora se encuentran en nuestra base de datos RDS, por lo utilizaremos esos para entrenar el modelo. Usando la lambda __send_query__ podemos hacer peticiones DML de tipo select.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e0fed2",
   "metadata": {},
   "source": [
    "## Procesamiento de datos pre TRAIN-TEST SPLIT\n",
    "\n",
    "Vamos a imputar las columnas de aquellas filas donde algun valor sea 0, pero si la fila tiene todos sus valores como 0 la eliminamos.\n",
    "\n",
    "Imputación con mediana para ser mas robusta a outlayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c10994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(clf, scaler):\n",
    "    payload = {\n",
    "        \"scaler\": scaler,            \n",
    "        \"classifier\": clf,    \n",
    "    }\n",
    "    joblib.dump(payload, \"./fast_api/model_and_scaler.joblib\")\n",
    "    \n",
    "def impute_zeros_with_median(df: pd.DataFrame, column: str, median_value: float) -> pd.DataFrame:\n",
    "    df[column] = df[column].apply(lambda x: median_value if x == 0 else x)\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_empty_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove rows where both 'vote_average' and 'budget' are 0.\n",
    "    \"\"\"\n",
    "    mask = ~((df[\"vote_average\"] == 0) & (df[\"budget\"] == 0))\n",
    "    return df[mask]\n",
    "\n",
    "\n",
    "def _add_success_column(df):\n",
    "    df_original = df.copy()\n",
    "    if 'revenue' in df.columns and 'budget' in df.columns:\n",
    "        df[\"success\"] = (df_original[\"revenue\"] > df_original[\"budget\"]) & (df_original[\"vote_average\"] > 7)\n",
    "    else:\n",
    "        raise ValueError(\"El DataFrame debe contener las columnas 'revenue' y 'budget'\")\n",
    "    \n",
    "    df[\"success\"] = [1 if success else 0 for success in df[\"success\"]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493f9a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = df_movies[[\"budget\", \"revenue\", \"runtime\", \"vote_average\", \"popularity\"]]\n",
    "data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf9ea67",
   "metadata": {},
   "source": [
    "#### Añadimos la columna de éxito segun la métrica que hemos definido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb148a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = _add_success_column(data_set)\n",
    "data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47ccce0",
   "metadata": {},
   "source": [
    "#### Eliminación de df[\"revenue\"]\n",
    "\n",
    "Mi métrica se ha calculado a partir de la popularidad, del budget y de revenue, reveneue no la podemos dejar pues sería ***data leakage***, pero las filas donde vote_average y budget sean 0 no nos valen para entrenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0180b23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = data_set.drop(columns=[\"revenue\"])\n",
    "data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5171c1",
   "metadata": {},
   "source": [
    "#### Eliminación de las filas sin información\n",
    "\n",
    "Consideramos filas inútiles si tenien ambas columnas budget y vote_average a 0 **A LA VEZ**. Si solo es una de ellas la imputaremos mas adelante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6500a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = drop_empty_rows(data_set)\n",
    "data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3998fe5",
   "metadata": {},
   "source": [
    "#### Imputación\n",
    "\n",
    "Calculamos la mediana de cada columna\n",
    ">IMPORTANTE: NO debemos calcular la mediana con los valores 0, ya que es un sesgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc492617",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEDIAN_BUDGET = data_set.loc[data_set[\"budget\"] > 0, \"budget\"].median()\n",
    "MEDIAN_RUNTIME = data_set.loc[data_set[\"runtime\"] > 0, \"runtime\"].median()\n",
    "MEDIAN_VOTE_AVERAGE = data_set.loc[data_set[\"vote_average\"] > 0, \"vote_average\"].median()\n",
    "MEDIAN_POPULARITY = data_set.loc[data_set[\"popularity\"] > 0, \"popularity\"].median()\n",
    "\n",
    "data_set = impute_zeros_with_median(data_set, \"budget\", MEDIAN_BUDGET)\n",
    "data_set = impute_zeros_with_median(data_set, \"runtime\", MEDIAN_RUNTIME)\n",
    "data_set = impute_zeros_with_median(data_set, \"vote_average\", MEDIAN_VOTE_AVERAGE)\n",
    "data_set = impute_zeros_with_median(data_set, \"popularity\", MEDIAN_POPULARITY)\n",
    "data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c8f657",
   "metadata": {},
   "source": [
    "## Procesamiento post TRAIN-TEST SPLIT\n",
    "\n",
    "Debedio a la gran diferencia de escala de datos necesitamos usar un StandarScaler para nuestro data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fda6e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = data_set.drop(\"success\", axis=1)\n",
    "y = data_set[\"success\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)\n",
    "\n",
    "# Escalado\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc6b9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e286d1",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cb531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=42, max_iter=10_000)\n",
    "\n",
    "clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c663e19a",
   "metadata": {},
   "source": [
    "#### Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cb60f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_scaled)\n",
    "y_proba = clf.predict_proba(X_test_scaled)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "f1score_ = f1_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average=None)\n",
    "roc_auc = roc_auc_score(y_test, y_proba[:,1])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba[:,1])\n",
    "\n",
    "print(acc)\n",
    "print(roc_auc)\n",
    "print(report)\n",
    "\n",
    "\n",
    "# Matriz de confusion\n",
    "confusion_matrix_ = sns.heatmap(data=cm, annot=True)\n",
    "plt.show()\n",
    "\n",
    "# ROC-AUC grafico\n",
    "sns.lineplot(x=fpr, y=tpr, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "sns.lineplot(x=[0,1], y=[0,1], color=\"red\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Curva ROC\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc1458f",
   "metadata": {},
   "source": [
    "#### Ahora guardamos el modelo en la carpeta de nuestra fast_api que subiremos a ec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14850f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(clf, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913f112b",
   "metadata": {},
   "source": [
    "## EC2 - Con Fast api\n",
    "\n",
    "Nos hemos construido una instancia ec2 que utilizaremos como servidor, que corra nuestro codigo python y escuche peticiones a los endpoints.\n",
    "\n",
    "Para ver la documentación en profundidad de la API:\n",
    "- **https://ec2-34-244-17-61.eu-west-1.compute.amazonaws.com:8000/docs**\n",
    "\n",
    "\n",
    "![Alt](./img/ec2.png \"Propiedades del EC2\")\n",
    "\n",
    "- ### /predict:\n",
    "    - Con la IA clasificadora que hemos entrenado espera datos de pelicula para predecir si tendrán exito. Nos devuelve una lista que representa la columna a predecir de los datos enviados. Ec2 necesita el fichero del modelo compilado para esto\n",
    "\n",
    "- ### /ask-text\n",
    "    - Paso 1: Le pasamos una pregunta en lenguaje natural como parámetro, con la libreria __google-genai__ usamos gemini para generarnos una sentencia SQL a partir de la pregunta\n",
    "    - Paso 2: Ejecutaremos con la lambda __send_quey__ que se conectará con __pycopg2__ a nuestra rds devolviendo la informacion de la sentencia.\n",
    "    - Paso 3: Le pasamos a gemini otra vez la información de la ejecucion de la query, para que transforme los datos a una explicación en lenguaje natural.\n",
    "\n",
    "- ### /ask-visual\n",
    "    - Paso 1: Mismo que en **/ask_text** pero tiene un parametro extra \"format\" que indica como queremos ver el resultado:\n",
    "        * **format = \"code\"**: el endooint entiende que estamos realizando la request desde código y nos devuelve codigo python para ejecutar y ver las graficas.\n",
    "        * **format = html**: pensado para cunado hacemos la peticion a través del navegador, mostrando la grafica en el propio navegador en codigo html\n",
    "    - Paso 2: Mismo que en **/ask_text**\n",
    "    - Paso 3: Se le pide por ultimo a gemini otra vez que desde los datos mostrados por la query en nuestro RDS. Generara codigo python que guardará en un archivo, si format = code este codigo será el resultado del endpoint, pero si format = html, este codigo guardado en un archivo temporal en ec2 será ejecutado, y generará una imagen que con una funcion envoltorio prepararemos y mostraremos con html\n",
    "\n",
    ">NOTA: Para ver en profundid el codigo de la nuestra api revisar la carpeta fast_api, esa carpeta se sube a ec2 con el comando \n",
    "```bash\n",
    "# Subir carpeta local 'mi_carpeta' a /home/ec2-user/ en la instancia\n",
    "scp -i /ruta/a/tu/key.pem -r ./mi_carpeta ec2-user@EC2_PUBLIC_IP:/home/ec2-user/\n",
    "\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
